{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computer science","text":""},{"location":"machine-learning/decision_tree/","title":"Decision tree","text":""},{"location":"machine-learning/decision_tree/#decision-tree","title":"Decision Tree","text":""},{"location":"machine-learning/decision_tree/#what-is-a-decision-tree","title":"What is a decision tree ?","text":"<p>A decision tree is a non-parametric supervised learning algorithm used for <code>classification</code> and <code>regression</code> tasks. It is a tree-like structure where each internal node represents a test on an attribute or feature,  each branch represents the outcome of the test, and each leaf node represents a class label or a numerical value that  corresponds to a prediction.</p> <p>An abstract example</p> <pre><code>\n    graph TD\n    A{Is feature X &gt;= 5?} --&gt;|yes| B{Is feature Y &gt;= 7?}\n    B --&gt;|yes| C[Class 1 - leaf node]\n    B --&gt;|no| D[Class 2 - leaf node]\n    A --&gt;|no| E{Is feature X &gt;= 3?}\n    E --&gt;|yes| F[Class 3 - leaf node]\n    E --&gt;|no| D[Class 4 - leaf node]\n\n</code></pre> <p>The goal of a decision tree algorithm is to learn a model that can accurately predict the target variable for new,  unseen data based on a set of training examples. The algorithm works by recursively splitting the data into subsets based on the values of the attributes or features, until each subset belongs to a single class or has reached a  stopping criterion.</p> <p>Can i use it for classification and regression?</p> <p>There are two main types of decision trees: classification trees and regression trees. Classification trees are used  for predicting categorical class labels, while regression trees are used for predicting continuous numerical values.</p> <p>How is built a decision tree ?</p> <p>To build a decision tree, the algorithm follows a top-down, greedy approach that recursively partitions the data based on the attribute that maximizes the information gain or minimizes the impurity at each node.  The information gain measures the reduction in entropy or impurity of the target variable by splitting the data on  a particular attribute, while the impurity measures the degree of uncertainty or randomness in the target variable.</p> <p>Once the tree is built, the algorithm can use it to make predictions on new, unseen data by traversing the tree from the root node down to a leaf node based on the values of the attributes. At each internal node, the algorithm checks  the value of the corresponding attribute, and follows the appropriate branch based on the outcome of the test.  At each leaf node, the algorithm outputs the class label or numerical value that corresponds to the prediction.</p>"},{"location":"machine-learning/decision_tree/#pros-cons","title":"Pros &amp; Cons","text":"<p>Tip</p> <p>You can easily visualize a decision tree, have a look to <code>sklearn.tree.plot_tree</code></p> <p>Decision trees have several advantages, including their interpretability, ease of use, and ability to handle both  categorical and numerical data. However, they also have some limitations, such as their tendency to overfit the  training data if not pruned or regularized, their sensitivity to small changes in the data, and their inability to  capture complex interactions between features.</p>"},{"location":"machine-learning/decision_tree/#deep-dive-in-the-algorithm","title":"Deep dive in the algorithm","text":"<p>How the algorithm work for a classification problem ?</p> <p>For classification problems, decision trees commonly use the Gini impurity or the information gain (also called entropy) to measure the quality of a split. The Gini impurity measures the probability of misclassifying a randomly chosen sample from the dataset, while the information gain measures the reduction in entropy (i.e., uncertainty) after the split.  The split that minimizes the impurity or maximizes the information gain is chosen.</p> <p>How the algorithm work for a regression problem ?</p> <p>For regression problems, decision trees commonly use the mean squared error (MSE) to measure the quality of a split. The MSE measures the variance of the target variable within each subset of the split.  The split that minimizes the MSE is chosen.</p> <p>Once the best split is chosen, the decision tree algorithm partitions the input data into two subsets based on the  split, and then recursively applies the same process to each subset until a stopping criterion is met. The stopping criterion could be a maximum depth of the tree, a minimum number of samples in each leaf node, or other criteria.</p> <p>A less abstract example</p> <p>Imagine you are developing an embedded system to monitor heart rate activity, what kind on information can you deduce  from the physiological data ?</p> <pre><code>\n    graph TD;\n    A((Heart Rate &lt;= 80))\n    A --&gt; |Yes| B((Age &lt;= 30))\n    A --&gt; |No| C((Age &lt;= 40))\n    B --&gt; |Yes| D(At Rest)\n    B --&gt; |No| E(Not At Rest)\n    C --&gt; |Yes| E\n    C --&gt; |No| F(At Rest)\n</code></pre>"},{"location":"machine-learning/exercice/","title":"Exercise","text":""},{"location":"machine-learning/exercice/#human-stress-classification-in-and-through-sleep","title":"Human stress classification in and through sleep","text":"<p>Goal</p> <p>The goal of the exercise is to build a model to classify the stress level of a patient given some physiological data. Four levels of stress are defined in the column <code>stress_level</code> as follow : </p> <ul> <li>0 - low/normal</li> <li>1 \u2013 medium low</li> <li>2 - medium</li> <li>3 - medium high</li> <li>4 - high</li> </ul> <p>It's a multiclass classification problem </p>"},{"location":"machine-learning/exercice/#input-data","title":"Input data","text":"<p>Here is an example of the data you'll use for the exercise.</p> <p>The dataset is composed of 631 samples</p> snoring_range respiration_rate body_temperature limb_movement_rate blood_oxygen_levels rapid_eye_movement hour_of_sleep heart_rate stress_level 93.8 25.68 91.84 16.6 89.84 99.6 1.84 74.2 3 91.64 25.104 91.552 15.88 89.552 98.88 1.552 72.76 3 60 20 96 10 95 85 7 60 1"},{"location":"machine-learning/exercice/#you-can-download-the-dataset-here-httpswetlt-3hhibkn0gv","title":"You can download the dataset here : https://we.tl/t-3hhIBKN0GV","text":""},{"location":"machine-learning/exercice/#instructions","title":"Instructions","text":""},{"location":"machine-learning/exercice/#tooling","title":"Tooling","text":"<p>Info</p> <p>You can use any library, any tool, it's up to you. Here are some ideas :</p> <ul> <li>pandas for data manipulation</li> <li>matplotlib / seaborn for data visualization</li> <li>sklearn : contains all the ML models and many functions you'll need</li> <li>Jupyter / GoogleCollab</li> </ul>"},{"location":"machine-learning/exercice/#1-data-exploration","title":"1 - Data exploration","text":"<ul> <li>First, load the heart rate data into a Pandas DataFrame (in Python) on GoogleColab or in a Jupyter notebook.</li> <li>Check for any missing or null values in the dataset and handle them appropriately.</li> <li>Visualize the distribution of each feature (i.e., snoring_range, respiration_rate, body_temperature,  limb_movement_rate, blood_oxygen_levels, rapid_eye_movement, hour_of_sleep, heart_rate)  using histograms or density plots to get a sense of the range and distribution of each variable.</li> <li>Examine the correlation between the features and the target variable (i.e., stress_level) to identify any highly correlated features.</li> </ul> <p>Tip</p> <p>Feel free to add any visualization that make sense, be creative !</p>"},{"location":"machine-learning/exercice/#2-data-preprocessing","title":"2 - Data preprocessing","text":"<ul> <li>Split the data into training and testing datasets (e.g., 80% training and 20% testing).</li> <li>Scale the features to a similar range to avoid bias in distance calculations. You can use a method like  min-max scaling or standard scaling. (You can try without !)</li> </ul>"},{"location":"machine-learning/exercice/#3-knn-model-training","title":"3 - KNN model training","text":"<p>Tip</p> <p>Have a look to <code>sklearn.neighbors.KNeighborsClassifier</code>...</p> <ul> <li>Use scikit-learn's KNeighborsClassifier to train a KNN model on the heart rate data.</li> <li>Choose an appropriate value of k (i.e., the number of nearest neighbors to consider) by trying different values and  evaluating the performance of the model using a validation dataset or cross-validation.</li> <li>Fit the model on the training dataset.</li> </ul>"},{"location":"machine-learning/exercice/#4-knn-model-evaluation","title":"4 - KNN Model evaluation","text":"<ul> <li>Evaluate the performance of the KNN model on the test dataset using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC score.</li> <li>Visualize the model's performance using a confusion matrix or ROC curve.</li> <li>Analyze the results and identify areas of improvement for the model.</li> </ul>"},{"location":"machine-learning/exercice/#5-decision-tree-model-training","title":"5 - Decision tree model training","text":"<p>same as <code>3 - KNN model training</code></p> <p>Tip</p> <p>Have a look to <code>sklearn.tree.DecisionTreeClassifier</code>...</p>"},{"location":"machine-learning/exercice/#6-decision-tree-model-evaluation","title":"6 - Decision tree model evaluation","text":"<p>same as <code>4 - KNN Model evaluation</code></p>"},{"location":"machine-learning/exercice/#7-compare-results","title":"7 - Compare results","text":"<p>Tip</p> <p>Just choose one of the model, justify your choice ! </p>"},{"location":"machine-learning/exercice/#bonus-compare-with-other-algorithms","title":"Bonus - Compare with other Algorithms","text":""},{"location":"machine-learning/exercice/#8-send-your-work","title":"8 - Send your work","text":"<p>Danger</p> <p>Your work (the notebook), must be shared in a github repository, you'll send the link to your repo by email.</p>"},{"location":"machine-learning/exercice/#thanks-for-the-dataset","title":"Thanks for the dataset","text":"<pre><code>L. Rachakonda, A. K. Bapatla, S. P. Mohanty, and E. Kougianos, \u201cSaYoPillow: Blockchain-Integrated Privacy-Assured IoMT \nFramework for Stress Management Considering Sleeping Habits\u201d, IEEE Transactions on Consumer Electronics (TCE), Vol. 67,\n No. 1, Feb 2021, pp. 20-29.\n\nL. Rachakonda, S. P. Mohanty, E. Kougianos, K. Karunakaran, and M. Ganapathiraju, \u201cSmart-Pillow: An IoT based Device \nfor Stress Detection Considering Sleeping Habits\u201d, in Proceedings of the 4th IEEE International Symposium on Smart \nElectronic Systems (iSES), 2018, pp. 161--166.\n</code></pre>"},{"location":"machine-learning/exercise-2/","title":"Exercise 2","text":""},{"location":"machine-learning/exercise-2/#instructions","title":"Instructions","text":"<p>We will use a dataset containing patient information like cholesterol, sugar in blood etc... The aim is to train a classification model to predict if a patient is exposed to heart attack or not.</p> <p>You can download the data here.  It will download a zip with two files, we will use <code>heart.csv</code> file</p> <p>Here a table summarizing the dataset:</p> Column Name Description Age Age of the patient Sex Sex of the patient exang Exercise induced angina (1 = yes; 0 = no) ca Number of major vessels (0-3) cp Chest Pain type: <ul><li>Value 1: typical angina</li><li>Value 2: atypical angina</li><li>Value 3: non-anginal pain</li><li>Value 4: asymptomatic</li></ul> trtbps Resting blood pressure (in mm Hg) chol Cholestoral in mg/dl fetched via BMI sensor fbs Fasting blood sugar &gt; 120 mg/dl (1 = true; 0 = false) rest_ecg Resting electrocardiographic results: <ul><li>Value 0: normal</li><li>Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV)</li><li>Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria</li></ul> thalach Maximum heart rate achieved target Target variable: 0 = less chance of heart attack, 1 = more chance of heart attack"},{"location":"machine-learning/exercise-2/#1-create-and-train-an-svm-classifier","title":"1 - Create and train an SVM classifier","text":"<p>In this part, simply train an SVM classifier on the heart dataset using its default parameter.</p> <p>Tip</p> <p>As for the previous exercise don't forget to:</p> <ul> <li>explore the data, provide few visualization, check feature importance</li> <li>clean it if necessary</li> <li>evaluate your model, provide the confusion matrix etc... </li> </ul> <p>Remember to check the CSV separator and set the <code>sep</code> parameter accordingly if you encounter issues when reading the  CSV file using Pandas. For example:</p> <pre><code>data = pd.read_csv(\"my_data.csv\", sep=\";\")\n</code></pre>"},{"location":"machine-learning/exercise-2/#2-change-manually-parameters","title":"2 - Change manually parameters","text":"<p>Train your model by changing C &amp; kernel. You can have a look to sklearn documentation to see what are the possible values. <pre><code>svm.SVC(kernel='poly', C=10)\n</code></pre></p>"},{"location":"machine-learning/exercise-2/#3-perform-gridsearch-to-find-optimal-parameters","title":"3 - Perform GridSearch to find optimal parameters","text":"<p>For this part, you need to apply grid search on your SVM classifier to find the best parameters You need to use <code>GridSearchCV</code> class to tune the following parameters: <code>C</code> &amp; <code>kernel</code>.</p> <p>Tip</p> <p>If you don't remember how does it work, have a look here : </p> <p>See an example</p> <p>You can import <code>GridSearchCV</code> as follows :  <pre><code>from sklearn.model_selection import GridSearchCV\n</code></pre></p>"},{"location":"machine-learning/exercise-2/#4-perform-randomizedsearch-to-find-optimal-parameters","title":"4 - Perform RandomizedSearch to find optimal parameters","text":"<p>For this part, you need to apply random search on your SVM classifier to find the best parameters You need to use <code>RandomizedSearchCV</code> class to tune the following parameters: <code>C</code> &amp; <code>kernel</code>.</p> <p>Tip</p> <p>You can import <code>RandomizedSearchCV</code> as follows :  <pre><code>from sklearn.model_selection import RandomizedSearchCV\n</code></pre></p>"},{"location":"machine-learning/exercise-2/#5-automate-multi-model-training-and-tuning","title":"5 - Automate multi-model training and tuning","text":"<p>The goal of this step is to define some python functions that will automatically train models and perform parameters tuning. </p> <p>Tip</p> <p>One idea is to associate a dictionary to each model, and pass it  to a function to run training on all your models, and output the model that perform the best. You can access the <code>best</code> model from a grid object as follows : <pre><code>grid = GridSearchCV(model, parameters, cv=5)\ngrid.fit(X_train, y_train)\ngrid.best_estimator_\n</code></pre></p> <pre><code>models_with_parameters = {\nsvm.SVC(): {'C': [0.1, 1, 100, 1000], ['kernel': 'linear', 'rbf', 'poly']},\nKNeighborsClassifier(): {'n_neighbors': [3, 5, 7],'weights': ['uniform', 'distance']}\n}\n</code></pre>"},{"location":"machine-learning/exercise-2/#6-send-your-github-project","title":"6 - Send your Github project","text":""},{"location":"machine-learning/exercise-3/","title":"Exercise 3","text":""},{"location":"machine-learning/exercise-3/#instructions","title":"Instructions","text":"<p>The aim of the exercise is to create an image classification model (CNN) in order to classify ... cats &amp; dogs image ! (It's a binary classification problem)</p> <p>You can download the dataset here</p> <p>The folder has the following architecture :</p> <pre><code>cats_and_dogs\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 cats\n\u2502   \u2514\u2500\u2500 dogs\n\u2514\u2500\u2500 test\n    \u251c\u2500\u2500 cats\n    \u2514\u2500\u2500 dogs\n</code></pre>"},{"location":"machine-learning/exercise-3/#1-exploration","title":"1 - Exploration","text":"<p>Before to deep dive into the convolutional neural network, have a look to the data. You must count the number of images per class (cats / dogs). Also, plot some images to ensure you are able to read them properly.</p>"},{"location":"machine-learning/exercise-3/#2-install-the-necessary-libraries","title":"2 - Install the necessary libraries","text":"<p>Info</p> <p>If you use google colab, installations are probably already done in the environment you are using.</p> <p>If you plan to develop on your local system, installed the necessary libraries : </p> <ul> <li>tensorflow</li> <li>matplotlib</li> <li>numpy</li> </ul> <p>We will use Keras, which is included in tensorflow</p>"},{"location":"machine-learning/exercise-3/#3-split-your-data-into-train-and-validation","title":"3 - Split your data into train and validation","text":"<p>As for other data we have already manipulated, we need to split our data into train &amp; test. The good news is that keras comes with some built-in preprocessing functions that help you to perform this operation. It also allows you to perform some data augmentation at the same time.</p> <p>You can have a look to the documentation</p> <p>Here is an example (only for train split): <pre><code>train_datagen = ImageDataGenerator(\nrescale=1./255,\nshear_range=0.2,\nzoom_range=0.2,\nhorizontal_flip=True\n)\ntrain_generator = train_datagen.flow_from_directory(\n'FOLDER/train',\ntarget_size=(150, 150),\nbatch_size=32,\nclass_mode='binary'\n)\n</code></pre></p> <p>Warning</p> <p>You also need to do it for test! However, you don't need to add some data augmentation for test set</p>"},{"location":"machine-learning/exercise-3/#4-build-train-your-model","title":"4 - Build &amp; Train your Model","text":"<p>Let's build a model, the goal is to test different architecture (add or remove some layer), varying hyperparameter etc...</p> <p>A very simple model would be:</p> <pre><code>model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',\noptimizer='rmsprop',\nmetrics=['accuracy'])\n</code></pre> <p>NB : The code here is only building the model, you need to search how to train it (maybe there is a fit method...)</p> <p>You can test the previous architecture, but you'll probably have very poor results. Try to build a more complex architecture  (with more Conv2D / MaxPooling) layer succession.  Also try to add some Dropout (look here) and change the optimizer (you can also change the maxpooling kernel size etc...)</p>"},{"location":"machine-learning/exercise-3/#5-evaluate-your-model","title":"5 - Evaluate your model","text":"<p>As mentioned in the course, you need to evaluate your cnn model as for any other model. As it's a classification problem, you, provide a confusion matrix. In addition, plot the curve representing accuracy/loss evolution over the epochs</p> <p>Very important : Download some images from internet, and test your model !</p>"},{"location":"machine-learning/exercise-3/#6-publish-your-code-on-github","title":"6 - Publish your code on Github","text":""},{"location":"machine-learning/intro/","title":"Introduction to Machine Learning","text":"<p>Machine Learning is a rapidly growing field in computer science that focuses on the development of algorithms and models that can learn from and make predictions on data. In this lecture, we will cover the basics of machine learning, including its applications, types of learning, and common algorithms.</p>"},{"location":"machine-learning/intro/#what-is-machine-learning","title":"What is Machine Learning?","text":"<p>Machine Learning (ML) is a subset of Artificial Intelligence (AI) that deals with the ability of a system to learn from data and improve its performance over time. The main goal of ML is to create models that can automatically learn patterns and relationships from data without being explicitly programmed.</p> <p></p>"},{"location":"machine-learning/intro/#and-what-about-deep-learning","title":"And what about Deep Learning ?","text":"<p>Deep learning is a subfield of Machine learning (when you heard about Neural network, it involves deep learning) one key difference between machine learning and deep learning is the level of abstraction involved in the feature  extraction process. In machine learning, features are typically manually extracted from the data and then used as inputs to the model. In deep learning, the model learns to extract the relevant features automatically,  as part of the training process.</p> <p>Question</p> <p>What is feature extraction ? </p> <p>Feature extraction is the process of selecting and transforming raw data into a set of features that can be used to  represent the data in a more meaningful way for a machine learning model. In machine learning, features are  essentially the measurable properties of the data that are used to make predictions or decisions.</p> <p></p>"},{"location":"machine-learning/intro/#why-it-became-so-popular-only-around-2010","title":"Why it became so popular only around 2010?","text":"<p>Machine learning has been around since the mid-20th century, but it wasn't until around 2010 that it became widely  popular. There are several reasons why this happened:</p> <ul> <li> <p>Big Data: The explosion of digital data in the 21st century has made it possible to train and test machine learning  models on massive amounts of data. This has made it easier to develop accurate models that can handle complex tasks.</p> </li> <li> <p>Advances in computing power: The development of powerful computers and the availability of cloud computing resources  have made it possible to process and analyze large amounts of data quickly and efficiently.</p> </li> <li> <p>Open-source software: The availability of open-source software frameworks like TensorFlow, PyTorch, and Scikit-learn  has made it easier for developers to experiment with machine learning algorithms and develop applications.</p> </li> <li> <p>Increased awareness: The growth of social media and the internet has led to increased awareness of the potential  applications of machine learning, and its ability to solve complex problems.</p> </li> </ul>"},{"location":"machine-learning/intro/#example-applications-of-machine-learning","title":"Example applications of Machine Learning","text":"<p>Machine Learning is being used in various applications across different industries. Some of the popular applications include:</p> <ul> <li>Image and Speech Recognition</li> <li>Natural Language Processing</li> <li>Fraud Detection</li> <li>Recommender Systems</li> <li>Predictive Maintenance</li> <li>Autonomous Vehicles</li> </ul>"},{"location":"machine-learning/intro/#types-of-machine-learning","title":"Types of Machine Learning","text":"<p>There are three main types of Machine Learning : </p>"},{"location":"machine-learning/intro/#supervised-learning","title":"Supervised Learning","text":"<p>In supervised learning, the model is trained on labeled data (input-output pairs). The goal is to learn a mapping function that can predict the output for new input data. Popular algorithms include Linear Regression, Logistic Regression, Decision Trees, Random Forests, and Neural Networks.</p>"},{"location":"machine-learning/intro/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>In unsupervised learning, the model is trained on unlabeled data. The goal is to learn the underlying structure of the data and identify patterns and relationships. Popular algorithms include Clustering, Principal Component Analysis (PCA), and Association Rule Learning.</p>"},{"location":"machine-learning/intro/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>In reinforcement learning, the model learns by interacting with the environment and receiving feedback in the form of rewards or punishments. The goal is to learn a policy that maximizes the cumulative reward over time. Popular algorithms include Q-Learning, SARSA, and Deep Reinforcement Learning.</p> <pre><code>\n    graph LR\n    A[Machine Learning] --&gt; B[Supervised Learning]\n    A --&gt; C[Unsupervised Learning]\n    A --&gt; D[Reinforcement Learning]\n    B --&gt; E[Regression]\n    B --&gt; F[Classification]\n    C --&gt; G[Clustering]\n    C --&gt; H[Dimensionality Reduction]\n</code></pre>"},{"location":"machine-learning/intro/#some-examples-foreach-category","title":"Some examples foreach category","text":""},{"location":"machine-learning/intro/#supervised-learning_1","title":"Supervised Learning:","text":"<p>Here are some examples of applications of supervised learning:</p> <ul> <li>Spam detection: In email filtering, the algorithm is trained on labeled data to classify emails as spam or not spam. </li> <li>Image recognition: In image classification, the algorithm is trained on labeled images to identify objects within images, such as people or animals. </li> </ul> <p>Data labeling information</p> <p>Maybe you know it, but when you're doing a captcha... you're labelling image</p> <p></p> <p>Facebook's photo tagging system is one example of how the company uses machine learning to identify faces in photos  and suggest tags to users. When users tag their friends in photos, it helps Facebook's algorithms learn and improve  their ability to identify faces and make more accurate suggestions in the future.</p> <p>Similarly, Twitter uses hashtags to train their algorithms. When users include hashtags in their tweets,  it helps Twitter's machine learning algorithms understand the context and topics being discussed.  This enables Twitter to make better recommendations to users about who to follow, what content to engage with,  and which ads to display.</p> <p></p> <ul> <li>Object detection is a computer vision task that involves identifying and localizing objects within an image or video. It is a more complex task than image classification because it not only requires identifying what objects are present in an image, but also where they are located within the image. Object detection algorithms are typically trained on labeled images that provide both the class label  (e.g. \"person\", \"car\", \"tree\", etc.) and the bounding box coordinates of each object in the image. </li> </ul> <p></p> <ul> <li>Fraud detection: In financial fraud detection, the algorithm is trained on labeled data to identify fraudulent transactions.</li> </ul>"},{"location":"machine-learning/intro/#unsupervised-learning_1","title":"Unsupervised Learning:","text":"<p>Here are some examples of applications of unsupervised learning:</p> <ul> <li>Customer segmentation: In marketing, unsupervised learning can be used to segment customers into groups based on similar behavior or characteristics.</li> <li>Anomaly detection: In cybersecurity, unsupervised learning can be used to identify unusual network activity or behavior that may be indicative of a security breach.</li> <li>Topic modeling: In natural language processing, unsupervised learning can be used to identify topics within a corpus of text documents.</li> </ul>"},{"location":"machine-learning/intro/#reinforcement-learning_1","title":"Reinforcement Learning:","text":"<p>Here are some examples of applications of reinforcement learning:</p> <ul> <li>Game playing: Reinforcement learning has been used to develop game-playing agents that can beat human experts at games such as chess and Go.</li> <li>Robotics: Reinforcement learning can be used to train robots to perform tasks such as grasping and manipulation of objects.</li> <li>Autonomous driving: Reinforcement learning can be used to train self-driving cars to navigate complex traffic scenarios.</li> </ul>"},{"location":"machine-learning/intro/#common-machine-learning-algorithms","title":"Common Machine Learning Algorithms","text":"<p>There are various algorithms used in Machine Learning, some of which include:</p> Algorithm Category Linear Regression Supervised Learning Logistic Regression Supervised Learning Decision Trees Supervised Learning Random Forest Supervised Learning Support Vector Machines (SVM) Supervised Learning k-Nearest Neighbors (k-NN) Supervised Learning Naive Bayes Supervised Learning K-Means Unsupervised Learning Hierarchical Clustering Unsupervised Learning Principal Component Analysis (PCA) Unsupervised Learning Apriori Unsupervised Learning QLearning Reinforcement Learning <p>Note</p> <p>This list contains only some of the common algorithm you'll met</p>"},{"location":"machine-learning/intro/#some-history","title":"Some history","text":""},{"location":"machine-learning/intro/#conclusion","title":"Conclusion","text":"<p>In conclusion, Machine Learning is an exciting field that has numerous applications and is rapidly growing. In this lecture, we have covered the basics of Machine Learning, including its applications, types of learning, and common algorithms. In the following lectures, we will dive deeper into each of these topics and learn how to apply them in real-world scenarios.</p>"},{"location":"machine-learning/knn/","title":"K-Nearest-neighbor (KNN)","text":"<p>K-nearest neighbors is a type of machine learning algorithm that can be used for both <code>regression</code> and <code>classification</code>  problems. The algorithm works by finding the <code>K</code> number of training examples that are closest to the new data point,  and then using the labels of those examples to make a prediction for the new data point.</p> <p>Classification with KNN</p> <p></p> <p>What is \"K\" ?</p> <p>The \"K\" in K-nearest neighbors refers to the number of neighbors that the algorithm uses to make its prediction.  For example, if K=3, then the algorithm will find the three training examples that are closest to the new data point, and use the labels of those examples to predict the label of the new data point.</p> <p>How does KNN works?</p> <p>The algorithm determines which training examples are closest to the new data point by calculating the distance between  the new data point and each of the training examples in the feature space. The most common distance metric used is  <code>Euclidean distance</code>, but other distance metrics can be used as well.</p> <p>Once the K nearest neighbors have been identified, the algorithm will use those neighbors to make a prediction for the  new data point. If the problem is a classification problem, the algorithm will use the mode (most common) label of the  K neighbors as the predicted label for the new data point. If the problem is a regression problem, the algorithm will  use the mean or median of the K neighbors as the predicted value for the new data point.</p> <p>Is the value of K important ?</p> <p>It's worth noting that the choice of K can have a significant impact on the performance of the algorithm. A small value  of K (e.g., K=1) can lead to overfitting, where the algorithm memorizes the training data and doesn't generalize well to new data. A large value of K (e.g., K=n, where n is the number of training examples) can lead to underfitting, where the algorithm doesn't capture the underlying structure of the data.</p> <p>Overall, K-nearest neighbors is a simple and effective machine learning algorithm that can be used for a variety of  problems. However, it can be sensitive to the choice of distance metric and the value of K, and may not perform as well  as other more sophisticated algorithms for certain types of data.</p>"},{"location":"machine-learning/knn/#type-of-distance","title":"Type of distance","text":"Distance Metric Formula Description Euclidean distance \\(\\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\\) Measures the straight-line distance between two points in a feature space. Manhattan distance \\(\\sum_{i=1}^{n}\\lvert x_i - y_i \\rvert\\) Measures the sum of the absolute differences between the coordinates of two points in a feature space. Minkowski distance (p=3) \\(\\sqrt[3]{\\sum_{i=1}^{n}(x_i - y_i)^3}\\) A generalization of both Euclidean distance and Manhattan distance that can be used for any value of p. Cosine similarity \\(\\frac{\\sum_{i=1}^{n}(x_i \\times y_i)}{\\sqrt{\\sum_{i=1}^{n}x_i^2} \\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\) Measures the cosine of the angle between two vectors in a feature space, which is commonly used for text classification and recommendation systems. Hamming distance \\(\\sum_{i=1}^{n}[x_i \\neq y_i]\\) Measures the number of positions at which two binary strings differ, which is commonly used for string matching and error correction."},{"location":"machine-learning/linear-regression/","title":"Linear regression","text":""},{"location":"machine-learning/linear-regression/#simple-linear-regression","title":"Simple linear regression","text":"<p>What is a simple linear regression?</p> <p>Simple linear regression is a statistical method used to model the relationship between two variables,  where one variable (the independent variable) is used to predict the other variable (the dependent variable).  The goal of simple linear regression is to find a line of best fit that summarizes the relationship between the two  variables. In other words, we want to find the equation of a line that comes as close as possible to passing through  all of the data points in the scatter plot of the two variables.</p>"},{"location":"machine-learning/linear-regression/#lets-start-by-an-example","title":"Let's start by an example","text":""},{"location":"machine-learning/linear-regression/#input-data","title":"Input data","text":"<p>Assuming we have a dataset which represents the given tips depending on the amount of the bill</p> <p>Info</p> <p>We already know some data</p> <p></p> <p>Info</p> <p>From this data, we can find <code>y = mx + b</code> that \"best fit\" the points</p> <p></p> <p>Info</p> <p>Now we can predict <code>tips_amount</code> given <code>total_bill</code></p> <p></p> <p>The equation for a straight line is given by:</p> \\[ {y = mx + b} \\] <p>Where :</p> <ul> <li>y is the dependent variable </li> <li>x is the independent variable</li> <li>m is the slope of the line</li> <li>b is the y-intercept (i.e., the value of y when x is zero).</li> </ul> <p>How to find the values of m and b?</p> <p>In simple linear regression, we use the method of least squares to find the values of <code>m</code> and <code>b</code> that minimize the  sum of the squared differences between the observed values of <code>y</code> and the predicted values of <code>y</code>  (i.e., the values of y predicted by the line of best fit).</p> <p>The least squares method involves the following steps:</p> <ul> <li>Compute the mean of the independent variable x and the mean of the dependent variable y.</li> <li> <p>Compute the slope of the line of best fit m using the formula: </p> <p><code>m = sum((xi - x_mean) * (yi - y_mean)) / sum((xi - x_mean)^2)</code></p> <p>Where : </p> <ul> <li>xi and yi are the values of the independent and dependent variables, respectively </li> <li>x_mean and y_mean are their respective means.</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Compute the y-intercept b using the formula:</p> <p><code>b = y_mean - m * x_mean</code></p> </li> </ul> <p>Once we have computed the values of m and b, we can use the equation <code>y = mx + b</code> to predict the value of y for any  given value of x.</p> <p>In the case of multiple linear regression, where there are multiple independent variables, the computation is a bit more complicated, but the basic idea is the same: we want to find the equation of a hyperplane that comes as close as  possible to passing through all of the data points in the scatter plot of the independent and dependent variables.  The method of least squares is also used to find the values of the coefficients (i.e., the slopes) of the hyperplane  that minimize the sum of the squared differences between the observed values of y and the predicted values of y.</p>"},{"location":"machine-learning/linear-regression/#implementation-example-with-sklearn","title":"Implementation example with Sklearn","text":""},{"location":"machine-learning/logistic-regression/","title":"Logistic regression","text":"<p>What is a logistic regression</p> <p>Logistic regression is a statistical method used for modeling the relationship between a binary dependent variable  (also known as the response or outcome variable) and one or more independent variables (also known as predictors  or explanatory variables). The goal of logistic regression is to predict the probability of the binary outcome  based on the values of the predictors.</p> <p>The dependent variable in logistic regression is typically binary (e.g., yes/no, 1/0, success/failure), although it can also be ordinal or nominal. The independent variables can be continuous, categorical, or a combination of both.</p> <p>The logistic regression model estimates the log odds (logit) of the binary outcome as a linear combination of the independent variables. The logit is then transformed into a probability using the logistic function, which produces a value between 0 and 1. This probability represents the predicted likelihood of the binary outcome given the values of the independent variables.</p> <p>Logistic regression is often used in classification tasks, such as predicting whether a customer will churn or not, whether an email is spam or not, or whether a patient will respond to a particular treatment or not. It can also be used for understanding the relationship between variables and exploring the effects of different factors on the binary outcome.</p> <p>The math behind</p> <p>The mathematical foundation of logistic regression is based on the logistic function, which is a type of sigmoid  function. The logistic function is defined as:</p> \\[f(z) = 1 / (1 + e^-z)\\] <p>where z is the linear combination of the independent variables and their associated coefficients, or weights. In other words, z = b0 + b1x1 + b2x2 + ... + bpxp, where b0 is the intercept or constant term and b1 to bp are the coefficients for the independent variables x1 to xp.</p>"},{"location":"machine-learning/logistic-regression/#the-iris-dataset-example","title":"The iris dataset example","text":"<p>The iris dataset is often used for educational purpose in machine learning.  It contains the following informations : </p> Column Description sepal_length Length of sepal (in centimeters) sepal_width Width of sepal (in centimeters) petal_length Length of petal (in centimeters) petal_width Width of petal (in centimeters) species The species of iris (Setosa, Versicolor, or Virginica) <p>Objective</p> <p>Given the four feature sepal_length, sepal_width, petal_length, petal_width, we want to build a model which is able  to predict the class (which species) based on new samples -&gt; It's a multi class classification problem</p>"},{"location":"machine-learning/logistic-regression/#lets-deep-dive-a-bit-in-the-data","title":"Let's deep dive a bit in the data","text":"<p>Info</p> <p>The example we will use contains 150 samples</p> <p></p>"},{"location":"machine-learning/logistic-regression/#now-lets-solve-this-classification-problem-with-the-logistic-regression","title":"Now let's solve this classification problem with the Logistic Regression","text":""},{"location":"machine-learning/model-evaluation/","title":"Model evaluation","text":""},{"location":"machine-learning/model-evaluation/#splitting-our-data-for-training-testing-and-validation","title":"Splitting our data for training, testing, and validation","text":"<p>Info</p> <p>Train-test-validation is a common approach used in machine learning to evaluate the performance of a model.  The approach involves dividing a dataset into three parts: a training set, a validation set, and a test set.</p>"},{"location":"machine-learning/model-evaluation/#the-training-set","title":"The training set","text":"<p>It is the portion of the dataset used to train the model. The model learns from the patterns and  relationships present in the training data and uses this knowledge to make predictions on new, unseen data.</p>"},{"location":"machine-learning/model-evaluation/#the-validation-set","title":"The validation set","text":"<p>It is used to evaluate the performance of the model during training.  As the model learns from the training data, its performance is evaluated on the validation set to assess whether  it is overfitting (i.e., memorizing the training data and performing poorly on new data) or underfitting  (i.e., not capturing the patterns and relationships present in the training data).</p>"},{"location":"machine-learning/model-evaluation/#the-test-set","title":"The test set","text":"<p>Tips</p> <p>You can think about the test set as <code>unseen data</code></p> <p>It is used to evaluate the performance of the model after it has been trained and tuned on the training and  validation sets. The test set provides an unbiased estimate of the model's performance on new, unseen data.</p> <p>The train-test-validation approach is important because it enables us to assess the quality of the model's predictions and make informed decisions about how to improve the model. By dividing the dataset into separate training, validation, and test sets, we can ensure that the model is not overfitting or underfitting and that it can generalize well to new, unseen data.</p>"},{"location":"machine-learning/model-evaluation/#cross-validation","title":"Cross-validation","text":"<p>What is cross validation ?</p> <p>Cross-validation is a technique used to evaluate the performance of a machine learning model on a limited dataset by  splitting the data into multiple subsets, training the model on some subsets, and evaluating its performance on the  remaining subsets.</p> <p>The basic idea behind cross-validation is to ensure that the model is not overfitting the data and can generalize well  to new data. It does this by simulating the process of testing the model on new data that it has not seen before.</p> <p>The most common form of cross-validation is k-fold cross-validation, where the data is divided into k equal-sized  subsets, or folds. The algorithm is then trained on k-1 folds and tested on the remaining fold. This process is  repeated k times, with each fold used exactly once as the testing data. The results from each fold are then averaged  to obtain an estimate of the model's performance.</p> <p>For example, in 5-fold cross-validation, the dataset would be split into five equal parts, with four parts used  for training and one part used for testing. This process is repeated five times, with each of the five parts used as the testing data once. The average of the results from the five rounds is then used as an estimate of the model's  performance.</p> <p></p> <p>One of the advantages of cross-validation is that it can provide a more accurate estimate of a model's performance  than a single train-test split. This is because the results are averaged over multiple rounds, reducing the variability of the estimate. It can also help to detect overfitting, as a model that overfits the training data will perform poorly on the testing data.</p> <p>Warning</p> <p>Cross-validation can also be computationally expensive, as it requires training and evaluating the model  multiple times. It may also be less appropriate for some datasets, such as those with highly imbalanced classes or  temporal dependencies.</p> <p>Overall, cross-validation is a powerful tool for evaluating the performance of a machine learning model and can help to improve the accuracy and generalization of the model.</p>"},{"location":"machine-learning/model-evaluation/#classification","title":"Classification","text":"<p>Tips</p> <p>In general we use a confusion matrix to visualize the error made by an algorithm in a classification problem it's not limited to binary classification.</p> Actual Positive Actual Negative Predicted Positive TP FP Predicted Negative FN TN Evaluation Metric Formula When to Use Accuracy (TP + TN) / (TP + TN + FP + FN) Balanced Precision TP / (TP + FP) Imbalanced Recall TP / (TP + FN) Imbalanced F1 Score 2 * (Precision * Recall) / (Precision + Recall) Imbalanced"},{"location":"machine-learning/model-evaluation/#regression","title":"Regression","text":"Evaluation Metric Formula When to Use Mean Squared Error (MSE) 1/n * \u2211(y_true - y_pred)^2 General Root Mean Squared Error (RMSE) \u221a(1/n * \u2211(y_true - y_pred)^2) General R-squared (R2) 1 - \u2211(y_true - y_pred)^2 / \u2211(y_true - y_true_mean)^2 General MAE (Mean Absolute Error) 1/n * \u2211                        (      y_true - y_pred ) General MAPE (Mean Absolute Percentage Error) 100% * 1/n * \u2211( y_true - y_pred) / y_true Specific"},{"location":"machine-learning/neural-network/","title":"Neural network","text":""},{"location":"machine-learning/neural-network/#a-simple-neural-network","title":"A simple neural network ...","text":"<p>Let's introduce the Perceptron ! (Aka single-layer neural  network)</p> <p>The basic idea behind a perceptron is to take an input vector of features, multiply each feature by a corresponding  weight, and then sum up these weighted inputs to produce an output. This output is then compared to a threshold value, and the perceptron outputs a 1 if the output is greater than or equal to the threshold, or a 0 if the output is less than the threshold.</p> <p>The weights in a perceptron are initially set to random values, and the perceptron is trained using a supervised learning algorithm called the perceptron learning rule. In this algorithm, the perceptron is presented with a training example, and if the perceptron's output is correct, no changes are made to the weights. If the output is incorrect, the weights are adjusted to bring the output closer to the correct value.</p> <p>Warning</p> <p>One important limitation of perceptrons is that they are only able to classify linearly separable data, meaning data that can be separated into two categories by a straight line or hyperplane. This limitation was addressed by the development of more complex neural network architectures, such as multi-layer perceptrons, that can handle non-linearly separable data.</p>"},{"location":"machine-learning/neural-network/#basic-architecture-of-a-multi-layer-perceptron","title":"Basic Architecture of a Multi-layer perceptron","text":""},{"location":"machine-learning/neural-network/#input-layer","title":"Input layer","text":"<p>The input layer is the first layer in a neural network and is responsible for receiving the input data. The input layer consists of neurons that correspond to the input features of the data. For example, if the input data has 10 features, then the input layer will have 10 neurons, with each neuron representing one input feature.</p>"},{"location":"machine-learning/neural-network/#hidden-layer","title":"Hidden layer","text":"<p>The hidden layer is a layer of neurons that lies between the input layer and the output layer. The hidden layer  processes the input data and performs computations to transform the data into a more meaningful representation. The number of hidden layers in a neural network can vary depending on the complexity of the problem being solved.</p>"},{"location":"machine-learning/neural-network/#output-layer","title":"Output layer","text":"<p>The output layer is the final layer in a neural network and is responsible for producing the output of the model. The output layer consists of neurons that correspond to the target variables of the data. For example, if the task is to classify images of dogs and cats, the output layer will have 2 neurons, with each neuron representing one of the classes.</p>"},{"location":"machine-learning/neural-network/#different-types-of-neural-network","title":"Different types of neural network","text":"<p>In this course, we will cover two types of neural network, however many architectures exists.</p> <p>Info</p> <p>Most of the architectures are used for specific uses cases. </p>"},{"location":"machine-learning/neural-network/#feedforward-neural-network","title":"Feedforward neural network","text":"<p>Tip</p> <p>This is the architecture we have seen previously</p> <p>This is the most basic type of neural network, also known as a multi-layer perceptron. It consists of an input layer, one or more hidden layers, and an output layer. The data flows from the input layer to  the output layer through the hidden layers. It is commonly used for classification and regression problems.</p>"},{"location":"machine-learning/neural-network/#convolutional-neural-network","title":"Convolutional neural network","text":"<p>CNN is a specialized type of neural network that is designed for processing and analyzing images and videos.  It uses convolutional layers to detect features in the input data and pooling layers to reduce the size of the output. It is commonly used for image and video recognition tasks.</p>"},{"location":"machine-learning/neural-network/#recurrent-neural-network","title":"Recurrent neural network","text":"<p>RNN is a type of neural network that is designed for processing sequential data,  such as time series data or natural language processing. It uses loops to process the input data and store information in its memory. It is commonly used for language modeling, speech recognition, and sentiment analysis.</p>"},{"location":"machine-learning/neural-network/#long-shot-term-memory","title":"Long shot-term memory","text":"<p>LSTM is a type of RNN that is designed for processing long-term dependencies. It uses memory cells and gates to store and control the flow of information in the network. It is commonly used for natural language processing and speech recognition tasks.</p> <p>Info</p> <p>For information GPT chat or BERT use a Transformer architecture</p>"},{"location":"machine-learning/neural-network/#how-does-neural-networks-learn","title":"How does neural networks \"learn\" ?","text":"<p>It depends on the kind of network ! In this section, we will discover the \"general\" methods, applied to some  specific type of models. </p> <p>Info</p> <p>As previously mentioned, we will focus on two architectures:</p> <ul> <li>Fastforward neural network </li> <li>Convolutional neural network</li> </ul> <p>Neural networks are trained through an optimization process that involves adjusting the weights and biases of the  network to minimize a loss function that measures the difference between the predicted output and the actual output.</p>"},{"location":"machine-learning/neural-network/#the-optimization-process","title":"The optimization process","text":"<p>Training a model is just minimizing a cost function (=loss function)</p> <p>The optimization process typically involves the following steps:</p> <ul> <li> <p>Forward pass: The input data is fed through the network, and the output is computed by applying a set of weights and biases to the input data.</p> </li> <li> <p>Compute loss: The difference between the predicted output and the actual output is measured using a loss function,  such as mean squared error or cross-entropy loss.</p> </li> <li> <p>Backward pass: The gradient of the loss function of the network is computed using the backpropagation algorithm (there are other algorithms).  The gradient measures the direction and magnitude of the change in the loss function that would result from a small  change in the weights and biases.</p> </li> <li> <p>Update weights: The weights and biases are updated in order to minimize the loss function : using the gradient descent</p> </li> </ul> <p>Tip</p> <p>The process of forward pass, compute loss, backward pass, and update weights is repeated for multiple iterations  until the loss function converges to a minimum. Each iteration is called an epoch.</p> <pre><code>graph LR\nA(Input Data) --&gt; B(Forward Pass)\nB --&gt; C(Compute Loss)\nC --&gt; D(Backward Pass)\nD --&gt; E(Update Weights)\nE --&gt; B\n</code></pre>"},{"location":"machine-learning/neural-network/#zoom-on-convolutional-neural-network","title":"Zoom on Convolutional neural network","text":"<p>Tip</p> <p>CNN is a type of model that is commonly used for image recognition, object detection, and other computer vision tasks.</p> <p>The key feature of CNNs is their ability to automatically learn features from raw input data, such as images,  by applying a series of convolutional layers. In these layers, the network applies a set of filters (also called kernels) to the input image to extract certain features, such as edges or shapes, that are relevant to the task at hand.</p>"},{"location":"machine-learning/neural-network/#you-said-convolution","title":"You said convolution ?","text":"<p>Convolution is a fundamental operation in image processing that involves applying a filter to an image to produce a new filtered image. The filter, also known as a kernel, is a matrix of values that is applied to each pixel in the  input image.</p> <p></p> <p>Each filter is applied to a small patch of the input image, and the dot product between the filter and the patch is  computed to produce a single output value. This output value is placed in a new feature map, at the same location  as the center of the patch in the input image. This process is repeated for every possible patch in the image.</p> <p>These features are then passed through additional layers such as pooling, activation, and fully connected layers to  classify the image or perform other relevant tasks.</p> <p></p>"},{"location":"machine-learning/neural-network/#layers-explanations","title":"Layers explanations","text":""},{"location":"machine-learning/neural-network/#pooling-layer","title":"Pooling layer","text":"<p>Tip</p> <p>The aim of this layer is tu reduce spatial dimension !</p> <p>The pooling layer is typically added after a convolutional layer in a CNN. Its purpose is to reduce the spatial  dimensions of the output feature maps produced by the convolutional layer while retaining the important features. This helps to reduce the number of parameters in the model and control overfitting. Max-pooling and average-pooling  are the most common types of pooling operations.</p> <p></p> <p>Max &amp; Average pooling illustration</p>"},{"location":"machine-learning/neural-network/#activation-layer","title":"Activation layer","text":"<p>Tip</p> <p>It introduice non-linearity !</p> <p>The activation layer introduces non-linearity into the model by applying a non-linear function to the output of a  convolutional or pooling layer. The most commonly used activation function is the Rectified Linear Unit (ReLU), which sets any negative values to zero and leaves positive values unchanged. Other activation functions include sigmoid, tanh, and softmax.</p>"},{"location":"machine-learning/neural-network/#fully-connected-layer","title":"Fully connected layer","text":"<p>Tip</p> <p>The layer responsible of classifying ! </p> <p>The fully connected layer is the final layer in a CNN and its purpose is to classify the image. The output of the  previous layer is flattened into a vector and fed into a fully connected layer, where each neuron in the layer is  connected to every neuron in the previous layer. The fully connected layer applies a weight to each input and passes  the output through an activation function to produce a class score. The most commonly used activation function in the  final layer is the softmax function, which converts the class scores into a probability distribution over the different  classes.</p> <p>Info</p> <p>In object detection, the fully connected layer may produce a set of bounding boxes and class scores for each object, while in semantic segmentation, the fully connected layer may produce a dense output tensor with the same dimensions as the input image.</p>"},{"location":"machine-learning/neural-network/#cnn-in-action-image-classification","title":"CNN in action - Image classification","text":"<p>Info</p> <p>For this example, we will use the mnist digit dataset (a dataset containing handwritten digit) - aka the hello  world of image classification </p>"},{"location":"machine-learning/neural-network/#import-the-necessary-libraries","title":"Import the necessary libraries","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n</code></pre>"},{"location":"machine-learning/neural-network/#import-the-data","title":"Import the data","text":"<p>Warning</p> <p>In a real life example, you'll read the data from your local system or any storage.</p> <pre><code>(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n</code></pre>"},{"location":"machine-learning/neural-network/#visualize-some-samples","title":"Visualize some samples","text":"<p><pre><code>fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(10, 6))\naxes = axes.flatten()\nfor i in range(len(axes)):\naxes[i].imshow(x_train[i], cmap='gray')\naxes[i].axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre> </p>"},{"location":"machine-learning/neural-network/#prepare-the-data","title":"Prepare the data","text":"<pre><code>num_classes = 10\ninput_shape = (28, 28, 1)\n# Load the data and split it between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n</code></pre> <p>Tip</p> <p>Here we're rescaling the image to range [0;1] because most activation functions used in neural networks  (such as ReLU activation) have the most sensitivity around zero and tend to saturate as their input gets larger.  By scaling the input images to a smaller range (such as [0, 1]), we ensure that the activation functions operate in their most sensitive regions, which can improve the overall performance of the model.</p>"},{"location":"machine-learning/neural-network/#create-the-model","title":"Create the model","text":"<pre><code>model = keras.Sequential(\n[\nkeras.Input(shape=input_shape),\nlayers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\nlayers.MaxPooling2D(pool_size=(2, 2)),\nlayers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\nlayers.MaxPooling2D(pool_size=(2, 2)),\nlayers.Flatten(),\nlayers.Dropout(0.5),\nlayers.Dense(num_classes, activation=\"softmax\"),\n]\n)\nmodel.summary()\n</code></pre> <p>Ok but why these values?</p> <p>Each filter in a Conv2D layer is responsible for detecting a specific feature or pattern in the input image,  and increasing the number of filters can help to capture a wider range of features. In the given model, the first Conv2D layer has 32 filters and captures basic features such as edges and corners, while the second Conv2D layer  with 64 filters captures more complex features such as shapes and textures.</p> <p>Reducing the kernel size in the second Conv2D layer can help to capture smaller details in the input image. The kernel size determines the receptive field of each filter in the Conv2D layer, and reducing the kernel size can increase the granularity of the features that are detected. In the given model, the first Conv2D layer has a kernel size of (3, 3), which captures larger patterns in the input image, while the second Conv2D layer with a smaller kernel size of (2, 2) captures smaller details.</p>"},{"location":"machine-learning/neural-network/#train-the-model","title":"Train the model","text":"<pre><code>batch_size = 128\nepochs = 15\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n</code></pre> <p>Warning</p> <p>At that point, you should monitor the performance of your model as for any other model we have previously seen. It's common to visualize the accuracy / loss on train and validation set. Also, you can provide the confusion matrix, and check other metrics like recall, F1-score, depending on the problem.</p> <p></p> <p>Tip</p> <p>kernel size, pool size, number of epoche, the type of optimizer ... They are all hyperparameters that you can tuned  when training your model ! </p> <p>Let's practice !</p>"},{"location":"machine-learning/parameter-tuning/","title":"Parameter tuning","text":""},{"location":"machine-learning/parameter-tuning/#what-is-parameter-tuning-and-why-it-is-important","title":"What is parameter tuning and why it is important?","text":"<p>Parameter tuning is the process of selecting the optimal values for the parameters of a machine learning model.  In most machine learning algorithms, there are various parameters that need to be specified to achieve optimal  performance on a particular dataset. These parameters can significantly affect the performance of the model and can  vary depending on the problem at hand.</p>"},{"location":"machine-learning/parameter-tuning/#how-to-apply-it","title":"How to apply it ?","text":"<p>Info</p> <p>There are different ways to perform parameter tuning, you can do it 'manually' by iterating over a list of parameters (in such a case, it's a gridsearch)</p>"},{"location":"machine-learning/parameter-tuning/#gridsearch","title":"GridSearch","text":"<p>The grid search approach would involve creating a list of values for each hyperparameter, and then evaluating the  performance of the model for all possible combinations of these values. The combination that results in the best  performance (e.g., highest accuracy, lowest error rate) is then chosen as the optimal set of hyperparameters.</p>"},{"location":"machine-learning/parameter-tuning/#example-with-knn","title":"Example with KNN","text":"<p>Assuming we want to tune parameters of a KNN classifier. Let's do it with the two following parameters : </p> <ul> <li><code>n_neighbors</code> the number of neighbors.  For this parameter we want to test 3 value : [3, 5, 7]</li> <li><code>weights</code> the weight function used in prediction.  For this parameter we want to test 2 value : ['uniform', 'distance']</li> </ul>"},{"location":"machine-learning/parameter-tuning/#the-stupid-way","title":"The stupid way","text":"<p>Warning</p> <p>Never do that way.</p> <p>The stupid way would be to manually create some classifier with all the combination...</p> <ul> <li><code>KNeighborsClassifier(n_neighbors=3, weights='uniform')</code> </li> <li><code>KNeighborsClassifier(n_neighbors=3, weights='distance')</code> </li> <li><code>KNeighborsClassifier(n_neighbors=5, weights='uniform')</code> </li> <li><code>KNeighborsClassifier(n_neighbors=5, weights='distance')</code> </li> <li><code>KNeighborsClassifier(n_neighbors=7, weights='uniform')</code> </li> <li><code>KNeighborsClassifier(n_neighbors=7, weights='distance')</code> </li> </ul>"},{"location":"machine-learning/parameter-tuning/#the-best-way-with-gridsearchcv-class","title":"The best way - With GridSearchCV class","text":"<p>A better way would to :</p> <ul> <li> <p>Define a <code>parameter grid</code> as follows : <pre><code>param_grid = {'n_neighbors': [3, 5, 7],\n'weights': ['uniform', 'distance']}\n</code></pre></p> </li> <li> <p>Define a Knn classifier without parameter: <pre><code>knn_classifier = KNeighborsClassifier()\n</code></pre></p> </li> </ul> <p>Tip</p> <p>Sklearn come with a built-in class <code>GridSearchCV</code>, design to perform parameter tuning and also CrossValidation !</p> <ul> <li>Pass the knn classifier and the parameter grid to <code>GridSearchCV()</code> object : <pre><code>grid_search = GridSearchCV(knn_classifier, param_grid, cv=5)\n</code></pre></li> <li>You can now train your model ! <pre><code>grid_search.fit(X_train, y_train)\n</code></pre></li> </ul> <p>Warning</p> <p>As you can see, with this approach, you will not call the fit method of your knn_classifier object directly but on the grid_search object</p> <p>A pseudo example</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create an instance of KNN classifier\nknn_classifier = KNeighborsClassifier()\n# Define a grid of hyperparameters to search over\nparam_grid = {'n_neighbors': [3, 5, 7],\n'weights': ['uniform', 'distance']}\n# Create a grid search object\ngrid_search = GridSearchCV(knn_classifier, param_grid, cv=5)\n# Fit the grid search object to the training data\ngrid_search.fit(X_train, y_train)\nprint(\"Best Hyperparameters: \", grid_search.best_params_)\nprint(\"Training Score: \", grid_search.best_score_)\nprint(\"Test Score: \", grid_search.score(X_test, y_test))\n</code></pre>"},{"location":"machine-learning/parameter-tuning/#randomsearch","title":"RandomSearch","text":"<p>Another very similar approach is called <code>RandomSearch</code>. The only difference with <code>GridSearch</code> is that you don't have to explicitly pass the values of the parameters, you have to pass the distribution of the parameters, eg : a range for an  integer</p>"},{"location":"machine-learning/parameter-tuning/#example","title":"Example","text":"<p>Have a look to GridSearch, this is almost the same. The only changes are the input parameters :  this time you pass a parameter_distribution dictionary, like :  <pre><code>param_dist = {'n_neighbors': randint(1, 10),\n'weights': ['uniform', 'distance']}\n</code></pre></p> <p>And use <code>RandomizedSearchCv</code> class instead of GridSearchCv. <pre><code>random_search = RandomizedSearchCV(knn, param_distributions=param_dist, cv=5, n_iter=20, random_state=42)\n</code></pre></p> <p>Info</p> <p>RandomizedSearchCV takes an additional parameter n_iter which corresponds to the number of iteration to perform.</p>"},{"location":"machine-learning/parameter-tuning/#other-methods","title":"Other methods","text":"<p>The two previous methods are very basics, however they can perform well on classic machine-learning problems.  There many other methods to perform parameter tuning. The choice will highly depends on the complexity of the problem you are trying to solve. In some cases (when the search space for hyperparameters is large), it's very time-consuming to evaluate models on each possibility, in those cases, it's better to use other techniques like Bayesian Optimization.</p>"},{"location":"machine-learning/prerequisites/","title":"Before developing","text":"<p>Warning</p> <p>Before creating a machine learning algorithm, there are several steps that should be taken to ensure that the  problem is well-defined, the data is appropriate and that the algorithm can be effectively trained</p>"},{"location":"machine-learning/prerequisites/#1-define-the-problem","title":"1 - Define the problem","text":"<p>Start by defining the problem you want to solve, the objectives. </p> <ul> <li>What is the goal of your algorithm? </li> <li>What kind of data will you be working with?</li> <li>What performance are we expecting ?</li> <li>Do I have data ? </li> </ul>"},{"location":"machine-learning/prerequisites/#2-collect-the-data-the-right-data","title":"2 - Collect the data... The right data","text":"<p>Gather the data you will need to train and test your algorithm. This might involve:</p> <ul> <li>scraping data from the web</li> <li>collecting data from sensors</li> <li>working with pre-existing data sets</li> </ul> <p>Danger</p> <p>In real-life application, the data itself is one thing on which you need to spend time :  </p> <ul> <li>You need to ensure that the data describe your real-life problem </li> <li>You need to ensure that its quality is good</li> <li>Are you able to produce the data you will use to train your model It doesn't make sense to have an algorithm that work well in laboratory condition, if you need to use it in  different condition</li> </ul> <p>Let's take an example :</p> <p>Example</p>"},{"location":"machine-learning/prerequisites/#assuming-you-want-to-train-a-classification-model-to-classify-eyes-diseases-with-a-camera-embedded-on-a-raspberry","title":"Assuming you want to train a classification model to classify eyes diseases, with a camera embedded on a raspberry.","text":"<p>After some research on google, you'll find some already labeled dataset like this :</p> <p></p> <p> </p> <p>But with your camera, you're only able to produce image like this : </p> <p></p> <p>It's completly out of scope, it doesn't make sense to train the algorithm on this dataset</p>"},{"location":"machine-learning/prerequisites/#what-can-i-do","title":"What can i do ?","text":"<p>You have mainly two solutions : </p> <ul> <li>Take some pictures of eyes with diseases you want to classify</li> <li>Scrap some data on internet and label it yourself</li> </ul>"},{"location":"machine-learning/prerequisites/#3-explore-the-data","title":"3 - Explore the data","text":"<p>Once you have your data, explore it to get a better understanding of what you're working with.  This might involve data visualization and basic statistical analysis.</p> <p>See an example</p>"},{"location":"machine-learning/prerequisites/#4-preprocess-the-data","title":"4 - Preprocess the data","text":"<p>Preprocessing the data involves transforming and cleaning the data so that it can be used by the machine learning  algorithm. This might involve feature engineering, data scaling, data normalization, data augmentation and data  cleaning.</p> What is data normalization ? <p>Data normalization is the process of rescaling the values of numeric features in a dataset to a common scale.  This is typically done to prevent features with large values from dominating the analysis and to ensure that all  features have equal weight. One common technique for normalization is called <code>min-max scaling</code>  which involves scaling the values of a feature to a range between 0 and 1 by subtracting the minimum value of the  feature and dividing by the range of the feature.</p> What is data scaling ? <p>Data scaling is a similar technique to normalization that involves transforming the values of numeric features  so that they have a mean of 0 and a standard deviation of 1. This is typically done to ensure that all features  have the same variance and that the data is centered around 0. One common technique for scaling is called  \"standardization,\" which involves subtracting the mean of the feature and dividing by its standard deviation.</p> What is data augmentation ? <p>Data augmentation is a technique used to increase the size of a training dataset by creating new, <code>synthetic</code> data points through transformations of existing data. The goal of data augmentation is to improve the  performance and generalization of machine learning models by exposing them to a wider range of variations in the data.</p> <p>Data augmentation can be applied to a wide range of data types, including images, audio, and text. For example, in image data augmentation, you can create new images by applying transformations such as rotation, translation, scaling,  flipping, and cropping to existing images. In audio data augmentation, you can apply transformations such as changing the pitch, tempo, or volume of existing audio recordings. In text data augmentation, you can create new text samples by replacing words with synonyms.</p> <p>Data augmentation is particularly useful in scenarios where the amount of available training data is limited. By creating new data points through data augmentation, you can effectively increase the size of the training set, which can help to prevent overfitting and improve the generalization of machine learning models.</p> <p>Example</p> <p></p> What is data cleaning ? <p>Data cleaning is a critical step in the data preprocessing pipeline that involves identifying and correcting errors,  inconsistencies, and missing or irrelevant data in a dataset. The goal of data cleaning is to prepare the data for  analysis or machine learning by ensuring that it is accurate, complete, and consistent.</p>"},{"location":"machine-learning/prerequisites/#data-cleaning-typically-involves-a-series-of-steps-which-may-include","title":"Data cleaning typically involves a series of steps, which may include:","text":""},{"location":"machine-learning/prerequisites/#missing-data-handling","title":"Missing data handling","text":"<p>If there are missing values in the dataset, data cleaning may involve imputing missing values  using techniques such as mean imputation, median imputation, or regression imputation.</p>"},{"location":"machine-learning/prerequisites/#outlier-detection-and-handling","title":"Outlier detection and handling","text":"<p>Outliers are data points that fall far outside the normal range of values in the  dataset. Data cleaning may involve detecting and handling outliers using techniques such as box plots, z-scores, or clustering.</p>"},{"location":"machine-learning/prerequisites/#data-formatting-and-type-conversion","title":"Data formatting and type conversion","text":"<p>Data cleaning may involve converting data types, such as converting categorical data to numerical data, or converting date formats to a standardized format.</p>"},{"location":"machine-learning/prerequisites/#removing-duplicates","title":"Removing duplicates","text":"<p>If there are duplicate records in the dataset, data cleaning may involve identifying and removing them to avoid biases in the analysis.</p>"},{"location":"machine-learning/prerequisites/#data-validation-and-verification","title":"Data validation and verification","text":"<p>Data cleaning may involve validating the data to ensure that it is accurate and  complete, such as cross-checking data against external sources or conducting manual spot-checks.</p>"},{"location":"machine-learning/resources/","title":"Some additional resources","text":"<ul> <li>https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/super-cheatsheet-machine-learning.pdf</li> <li>https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-machine-learning-tips-and-tricks.pdf</li> <li>Understand CNN : https://poloclub.github.io/cnn-explainer/</li> </ul>"},{"location":"machine-learning/snippets/","title":"Code snippets","text":"<p>In this page, you will find some basic code snippets that can help you in your machine-learning journey</p> Python Code Snippet Description <code>import pandas as pd</code> Importing the Pandas library as <code>pd</code> <code>import numpy as np</code> Importing the NumPy library as <code>np</code> <code>df = pd.read_csv(\"your_file.csv\", sep=\";\")</code> Reading a CSV file and creating a Pandas DataFrame with a specified separator <code>arr = df.values</code> Creating a NumPy array from a Pandas DataFrame <code>df = pd.DataFrame(arr, columns=['col1', 'col2', 'col3'])</code> Creating a Pandas DataFrame from a NumPy array with specified column names <code>df.head(n)</code> Displaying the first <code>n</code> rows of a Pandas DataFrame <code>df.describe()</code> Displaying summary statistics of a Pandas DataFrame <code>df.dropna(axis=0/1)</code> Dropping rows or columns with missing values from a Pandas DataFrame <code>df.fillna(df.mean(), inplace=True)</code> Imputing missing values with the mean of the column in a Pandas DataFrame <code>df = (df - df.mean()) / df.std()</code> Standardizing the values in a Pandas DataFrame <code>df = pd.get_dummies(df, columns=['cat_var'])</code> Creating dummy variables for categorical variables in a Pandas DataFrame <code>from sklearn.model_selection import train_test_split</code> Importing the <code>train_test_split</code> function from scikit-learn <code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</code> Splitting a Pandas DataFrame into training and testing sets <code>from sklearn.tree import DecisionTreeClassifier</code> Importing the <code>DecisionTreeClassifier</code> class from scikit-learn <code>clf = DecisionTreeClassifier()</code> Creating a Decision Tree Classifier object <code>clf.fit(X_train, y_train)</code> Training a Decision Tree Classifier on a training set <code>clf.score(X_test, y_test)</code> Evaluating the accuracy of a trained Decision Tree Classifier on a testing set <code>from sklearn.ensemble import RandomForestClassifier</code> Importing the <code>RandomForestClassifier</code> class from scikit-learn <code>from sklearn.model_selection import GridSearchCV</code> Importing the <code>GridSearchCV</code> class from scikit-learn <code>rfc = RandomForestClassifier()</code> Creating a Random Forest Classifier object <code>param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 5, 10]}</code> Defining a parameter grid for hyperparameter tuning <code>clf = GridSearchCV(rfc, param_grid, cv=5)</code> Creating a Grid Search object <code>clf.fit(X_train, y_train)</code> Fitting the Grid Search object on a training set <code>clf.best_params_</code> Displaying the best hyperparameters found by the Grid Search <code>confusion_matrix(y_true, y_pred)</code> Creating a confusion matrix using scikit-learn <code>plt.matshow(corr_matrix)</code> Creating a correlation matrix plot using Matplotlib <code>importance = clf.feature_importances_</code> Creating a feature importance array from a trained scikit-learn model <code>indices = np.argsort(importance)[::-1]</code> Sorting feature indices in descending order of importance <code>plt.bar(range(X_train.shape[1]), importance[indices])</code> Creating a bar plot of feature importances using Mat"},{"location":"machine-learning/svm/","title":"SVM","text":"<p>Support Vector machines (SVMs) are a type of machine learning algorithm used for both classification and regression.</p> <p>The basic concept behind SVM is to find the best possible boundary or hyperplane that separates the data points into  different classes (for a classification problem).</p>"},{"location":"machine-learning/svm/#how-does-it-work","title":"How does it work ?","text":""},{"location":"machine-learning/svm/#linear-problems","title":"Linear problems","text":"<p>Let's start with an example</p> <p>Assuming we've the following data, which represents a two-dimensional binary problem: </p> <p></p> <p>It's super easy to draw a hyperplane that separate the data...</p> A working hyperplane A better hyperplane <p>Question</p> <p>But how to choose the 'best'? hyperplane</p> <p>The hyperplane is chosen such that the margin or distance between the hyperplane and the closest  data points from each class is maximized.</p> <p>In a Support Vector Machine algorithm, the margins are the boundaries between the support vectors  (the closest points to the decision boundary) and the rest of the data points</p> A working hyperplane A better hyperplane <p>Info</p> <p>When no points are allowed to cross the margins, we are speaking about <code>hard margin classifier</code> (right figure). This only works when the data is linearly separable, which is not the case in many problems. On the contrary,  we are talking about <code>soft margin classifier</code> (left figure).</p> <p>Soft margin allows some data points to be misclassified by the hyperplane, but only by a certain amount, which is controlled by a parameter called C (in <code>scikit-learn</code>). The larger the value of C, the more strict the margin becomes, meaning fewer data points are allowed to be misclassified. Conversely, a smaller value of C allows more misclassified data  points but can lead to a larger margin.</p> <p>The advantage of soft margin is that it can handle more complex datasets that are not linearly separable. By  allowing some margin of error, the SVM algorithm can still find a hyperplane that separates the classes as well as  possible. However, the choice of the value of C is important, as it can affect the performance of the algorithm  on different datasets.</p> <p>C is an hyperparameter</p>"},{"location":"machine-learning/svm/#non-linear-problems","title":"Non-linear problems","text":"<p>Let's take a simple dataset... which is non-linearly separable.</p> <p>Question</p> <p>How to find the hyperplan that separate the two classes ?</p> <p></p> <p>Here, we are dealing with a 1 feature (X) dataset</p>"},{"location":"machine-learning/svm/#making-a-dataset-linearly-separable","title":"Making a dataset linearly separable","text":"<p>One approach to make a non-linearly separable data set, into a separable one is to create additional features derived  from the original one. In our example, it means we will transform one feature, to two features. In other words, we are increasing the dimensionality of the feature space.</p> <p>Warning</p> <p>By increasing the dimensionality of the feature space, we are introducing some additional computations. It can be a problem on large datasets.</p>"},{"location":"machine-learning/svm/#polynomial-features","title":"Polynomial features","text":"<p>Now, it's really easy to find the hyperplane that separate the two classes </p>"},{"location":"machine-learning/svm/#radial-basis-function","title":"Radial basis function","text":"<p>Let's take another example, a dataset which contains two features : </p> <p>Credit to James Thorn</p> <p>Another approach (very common using SVM) is to use radial basis function to increase dimensionality. The RBF is defined as:</p> \\[ K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2) \\] <p>By applying this transformation, we're increasing the dimension of our dataset (a third dimension is added).</p> <p>Let's visualize the result:</p> <p></p> <p>Info</p> <p>We can see that the data is now linearly separable by an horizontal plan, around <code>r=0.7</code></p>"},{"location":"machine-learning/svm/#the-kernel","title":"The kernel","text":"<p>We have previously seen that performing transformations of a feature space to a higher dimension can lead to extensive computation. What is the solution ? </p> <p>Let's introduce the kernel trick !</p> <p>The kernel trick is a technique used to implicitly transform the input data into a higher-dimensional space without  actually computing the transformed data points. In SVMs, the kernel function is used to transform the input data into a  higher-dimensional space where the data points may become linearly separable. Instead of explicitly computing the  transformed data points, the kernel function is used to compute the dot product between pairs of data points in the  transformed space, without actually computing the transformed data points.</p> <p>Tip</p> <p>SVMs can efficiently work with high-dimensional data, without explicitly computing the transformed data points.  This makes SVMs computationally efficient and allows them to handle large amounts of data.</p>"},{"location":"machine-learning/svm/#in-practice","title":"In practice","text":"<p>Let's take a simple classification example.</p> <p>For the example, I will generate some data using sklearn</p>"},{"location":"machine-learning/svm/#step-1-importing-the-necessary-modules","title":"Step 1 - Importing the necessary modules","text":"<pre><code>from sklearn import svm\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"machine-learning/svm/#step-2-create-fake-date-you-dont-need-to-remember-this","title":"Step 2 - Create fake date (you don't need to remember this)","text":"<pre><code>X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)\n</code></pre>"},{"location":"machine-learning/svm/#step-3-visualize-the-data","title":"Step 3 - Visualize the data","text":"<pre><code># Plot all the data points\nplt.scatter(X[:, 0], X[:, 1], c=[colors[c] for c in y])\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('SVM Classifier')\n# Add a legend for the class labels\nhandles = [plt.scatter([], [], c=colors[c], label='Class {}'.format(c)) for c in colors]\nplt.legend(handles=handles)\nplt.show()\n</code></pre>"},{"location":"machine-learning/svm/#step-3-creating-an-instance-of-svm-classifier-and-fit-it-to-the-data","title":"Step 3 - Creating an instance of SVM Classifier and fit it to the data","text":"<pre><code>svm_classifier = svm.SVC(kernel='linear', C=1)\nsvm_classifier.fit(X, y)\n</code></pre> <p>Info</p> <p>As you can see, it's very similar to what we have done with other type of models.</p>"},{"location":"machine-learning/svm/#step-4-make-prediction-on-new-data","title":"Step 4 - Make prediction on new data","text":"<pre><code>new_data = [[5.0, 0.5], [1, 4]]\npredicted_labels = svm_classifier.predict(new_data)\n</code></pre>"},{"location":"machine-learning/svm/#step-5-lets-visualize-our-predictions","title":"Step 5 - Let's visualize our predictions","text":"<p>By adding the following code to our previous visualization code, you can see the new datapoints and their predicted  labels <pre><code>for i in range(len(new_data)):\nplt.scatter(new_data[i][0], new_data[i][1], s=80, color=colors[predicted_labels[i]], edgecolors='g', linewidth=2)\n</code></pre></p> <p></p>"},{"location":"machine-learning/svm/#additional-information","title":"Additional information","text":"<p>As you can see, you can pass multiple parameters to the SVC class:</p> <ul> <li><code>kernel</code>, default = 'rbf'</li> <li><code>C</code>, default = <code>1</code></li> </ul> <p>Those two hyperparameters are particularly important when building an SVM model. To choose their values, you can perform <code>parameter tuning</code>.</p> <p>To tune these hyperparameters, one approach is to use a grid search, which involves trying out a range of values for  each parameter and selecting the combination that gives the best performance on a validation set. Another approach is to use a randomized search, which randomly samples from the hyperparameter space, and again selects the combination that  gives the best performance on a validation set</p>"},{"location":"machine-learning/tooling/","title":"Tooling we will use","text":""},{"location":"machine-learning/tooling/#language","title":"Language","text":"<p>Python</p>"},{"location":"machine-learning/tooling/#a-platform","title":"A platform","text":"<p>Google colab</p> <p>It's a cloud-based platform for developing and running machine learning models. It is a free service provided by Google  that allows users to write and run Python code in a Jupyter notebook environment, with access to powerful computing  resources and pre-installed libraries and frameworks for machine learning.</p> <p>Go to google colab</p>"},{"location":"machine-learning/tooling/#machine-learning-libraries","title":"Machine learning libraries","text":"<ul> <li>scikit-learn</li> <li>TensorFlow</li> <li>Keras</li> </ul> <p>These libraries provide pre-built functions and models for various machine learning tasks, making it easier to develop  machine learning models.</p>"},{"location":"machine-learning/tooling/#data-visualization-libraries","title":"Data visualization libraries","text":"<ul> <li>matplotlib </li> <li>seaborn</li> </ul>"},{"location":"machine-learning/tooling/#data-manipulation-libraries","title":"Data manipulation libraries","text":"<ul> <li>pandas</li> <li>numpy</li> </ul>"},{"location":"sorting-algorithm/bubble-sort/","title":"Bubble sort","text":""},{"location":"sorting-algorithm/bubble-sort/#bubble-sort","title":"Bubble sort","text":"<p>Bubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.</p> <p>Warning</p> <p>Bubble sort is not suitable to sort large datasets</p> <p>Implementation</p> GoPython <pre><code>func bubbleSort(integers []int) []int {\nnbElements := len(integers)\nfor i := 0; i &lt; nbElements-1; i++ {\nfor j := 0; j &lt; nbElements-i-1; j++ {\nif integers[j] &gt; integers[j+1] {\nintegers[j], integers[j+1] = integers[j+1], integers[j]\n}\n}\n}\nreturn integers\n}\n</code></pre> <pre><code>def bubble_sort():\npass\n</code></pre> <p>Time complexity</p> \\[ {O(n^2)} \\] <p>Space complexity \\({O(1)}\\)</p>"},{"location":"sorting-algorithm/insertion-sort/","title":"Insertion sort","text":""},{"location":"sorting-algorithm/insertion-sort/#insertion-sort","title":"Insertion sort","text":"<p>Insertion sort is a simple sorting algorithm that builds the final sorted array one item at a time. It iterates through an input list, compares adjacent elements and inserts the current element into the correct position in the sorted list.</p> <p>Warning</p> <p>Insertion sort is not suitable to sort large datasets</p> <p>Implementation</p> GoPython <pre><code>func insertionSort(integers []int) []int {\nnbElements := len(integers)\nfor i := 0; i &lt; nbElements; i++ {\nvalueToInsert := integers[i]\ncurrentPosition := i\nfor currentPosition &gt; 0 &amp;&amp; integers[currentPosition-1] &gt; valueToInsert {\nintegers[currentPosition] = integers[currentPosition-1]\ncurrentPosition = currentPosition - 1\n}\nintegers[currentPosition] = valueToInsert\n}\nreturn integers\n}\n</code></pre> <pre><code>def insertion_sort():\npass\n</code></pre> <p>Time complexity</p> <p>Space complexity</p>"},{"location":"sorting-algorithm/selection-sort/","title":"Selection sort","text":""},{"location":"sorting-algorithm/selection-sort/#selection-sort","title":"Selection sort","text":"<p>Selection sort is an in-place comparison sorting algorithm that divides the input list into two parts: the sublist of items already sorted, and the sublist of items remaining to be sorted. It finds the minimum element in the unsorted sublist and swaps it with the leftmost unsorted element, moving the sublist boundaries one element to the right.</p> <p>Warning</p> <p>Selection sort is not suitable to sort large datasets</p>"},{"location":"sorting-algorithm/selection-sort/#implementation","title":"Implementation","text":"GoPython <pre><code>func selectionSort(integers []int) []int {\nnbElements := len(integers)\nfor i := 0; i &lt; nbElements-1; i++ {\nminIndex := i\nfor j := i; j &lt; nbElements; j++ {\nif integers[j] &lt; integers[minIndex] {\nminIndex = j\n}\n}\nintegers[i], integers[minIndex] = integers[minIndex], integers[i]\n}\nreturn integers\n}\n</code></pre> <pre><code>def selection_sort():\npass\n</code></pre>"},{"location":"sorting-algorithm/selection-sort/#time-complexity","title":"Time complexity","text":""},{"location":"sorting-algorithm/selection-sort/#space-complexity","title":"Space complexity","text":"<p>Note</p> <p>Selection sort is useful when memory usage is a concern, as it is an in-place sorting algorithm that does not require additional memory. It is also useful when you only need to sort a small number of items, and performance is not a critical concern. Selection sort can be used when the list is partially sorted or when the order of equal elements is not important.</p>"}]}