{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computer science","text":""},{"location":"machine-learning/decision_tree/","title":"Decision tree","text":""},{"location":"machine-learning/decision_tree/#decision-tree","title":"Decision Tree","text":""},{"location":"machine-learning/decision_tree/#what-is-a-decision-tree","title":"What is a decision tree ?","text":"<p>A decision tree is a non-parametric supervised learning algorithm used for <code>classification</code> and <code>regression</code> tasks. It is a tree-like structure where each internal node represents a test on an attribute or feature,  each branch represents the outcome of the test, and each leaf node represents a class label or a numerical value that  corresponds to a prediction.</p> <p>An abstract example</p> <pre><code>\n    graph TD\n    A{Is feature X &gt;= 5?} --&gt;|yes| B{Is feature Y &gt;= 7?}\n    B --&gt;|yes| C[Class 1 - leaf node]\n    B --&gt;|no| D[Class 2 - leaf node]\n    A --&gt;|no| E{Is feature X &gt;= 3?}\n    E --&gt;|yes| F[Class 3 - leaf node]\n    E --&gt;|no| D[Class 4 - leaf node]\n\n</code></pre> <p>The goal of a decision tree algorithm is to learn a model that can accurately predict the target variable for new,  unseen data based on a set of training examples. The algorithm works by recursively splitting the data into subsets based on the values of the attributes or features, until each subset belongs to a single class or has reached a  stopping criterion.</p> <p>Can i use it for classification and regression?</p> <p>There are two main types of decision trees: classification trees and regression trees. Classification trees are used  for predicting categorical class labels, while regression trees are used for predicting continuous numerical values.</p> <p>How is built a decision tree ?</p> <p>To build a decision tree, the algorithm follows a top-down, greedy approach that recursively partitions the data based on the attribute that maximizes the information gain or minimizes the impurity at each node.  The information gain measures the reduction in entropy or impurity of the target variable by splitting the data on  a particular attribute, while the impurity measures the degree of uncertainty or randomness in the target variable.</p> <p>Once the tree is built, the algorithm can use it to make predictions on new, unseen data by traversing the tree from the root node down to a leaf node based on the values of the attributes. At each internal node, the algorithm checks  the value of the corresponding attribute, and follows the appropriate branch based on the outcome of the test.  At each leaf node, the algorithm outputs the class label or numerical value that corresponds to the prediction.</p>"},{"location":"machine-learning/decision_tree/#pros-cons","title":"Pros &amp; Cons","text":"<p>Tip</p> <p>You can easily visualize a decision tree, have a look to <code>sklearn.tree.plot_tree</code></p> <p>Decision trees have several advantages, including their interpretability, ease of use, and ability to handle both  categorical and numerical data. However, they also have some limitations, such as their tendency to overfit the  training data if not pruned or regularized, their sensitivity to small changes in the data, and their inability to  capture complex interactions between features.</p>"},{"location":"machine-learning/decision_tree/#deep-dive-in-the-algorithm","title":"Deep dive in the algorithm","text":"<p>How the algorithm work for a classification problem ?</p> <p>For classification problems, decision trees commonly use the Gini impurity or the information gain (also called entropy) to measure the quality of a split. The Gini impurity measures the probability of misclassifying a randomly chosen sample from the dataset, while the information gain measures the reduction in entropy (i.e., uncertainty) after the split.  The split that minimizes the impurity or maximizes the information gain is chosen.</p> <p>How the algorithm work for a regression problem ?</p> <p>For regression problems, decision trees commonly use the mean squared error (MSE) to measure the quality of a split. The MSE measures the variance of the target variable within each subset of the split.  The split that minimizes the MSE is chosen.</p> <p>Once the best split is chosen, the decision tree algorithm partitions the input data into two subsets based on the  split, and then recursively applies the same process to each subset until a stopping criterion is met. The stopping criterion could be a maximum depth of the tree, a minimum number of samples in each leaf node, or other criteria.</p> <p>A less abstract example</p> <p>Imagine you are developing an embedded system to monitor heart rate activity, what kind on information can you deduce  from the physiological data ?</p> <pre><code>\n    graph TD;\n    A((Heart Rate &lt;= 80))\n    A --&gt; |Yes| B((Age &lt;= 30))\n    A --&gt; |No| C((Age &lt;= 40))\n    B --&gt; |Yes| D(At Rest)\n    B --&gt; |No| E(Not At Rest)\n    C --&gt; |Yes| E\n    C --&gt; |No| F(At Rest)\n</code></pre>"},{"location":"machine-learning/exercice/","title":"Exercise","text":""},{"location":"machine-learning/exercice/#human-stress-classification-in-and-through-sleep","title":"Human stress classification in and through sleep","text":"<p>Goal</p> <p>The goal of the exercise is to build a model to classify the stress level of a patient given some physiological data. Four levels of stress are defined in the column <code>stress_level</code> as follow : </p> <ul> <li>0 - low/normal</li> <li>1 \u2013 medium low</li> <li>2 - medium</li> <li>3 - medium high</li> <li>4 - high</li> </ul> <p>It's a multiclass classification problem </p>"},{"location":"machine-learning/exercice/#input-data","title":"Input data","text":"<p>Here is an example of the data you'll use for the exercise.</p> <p>The dataset is composed of 631 samples</p> snoring_range respiration_rate body_temperature limb_movement_rate blood_oxygen_levels rapid_eye_movement hour_of_sleep heart_rate stress_level 93.8 25.68 91.84 16.6 89.84 99.6 1.84 74.2 3 91.64 25.104 91.552 15.88 89.552 98.88 1.552 72.76 3 60 20 96 10 95 85 7 60 1"},{"location":"machine-learning/exercice/#you-can-download-the-dataset-here-httpswetlt-3hhibkn0gv","title":"You can download the dataset here : https://we.tl/t-3hhIBKN0GV","text":""},{"location":"machine-learning/exercice/#instructions","title":"Instructions","text":""},{"location":"machine-learning/exercice/#tooling","title":"Tooling","text":"<p>Info</p> <p>You can use any library, any tool, it's up to you. Here are some ideas :</p> <ul> <li>pandas for data manipulation</li> <li>matplotlib / seaborn for data visualization</li> <li>sklearn : contains all the ML models and many functions you'll need</li> <li>Jupyter / GoogleCollab</li> </ul>"},{"location":"machine-learning/exercice/#1-data-exploration","title":"1 - Data exploration","text":"<ul> <li>First, load the heart rate data into a Pandas DataFrame (in Python) on GoogleColab or in a Jupyter notebook.</li> <li>Check for any missing or null values in the dataset and handle them appropriately.</li> <li>Visualize the distribution of each feature (i.e., snoring_range, respiration_rate, body_temperature,  limb_movement_rate, blood_oxygen_levels, rapid_eye_movement, hour_of_sleep, heart_rate)  using histograms or density plots to get a sense of the range and distribution of each variable.</li> <li>Examine the correlation between the features and the target variable (i.e., stress_level) to identify any highly correlated features.</li> </ul> <p>Tip</p> <p>Feel free to add any visualization that make sense, be creative !</p>"},{"location":"machine-learning/exercice/#2-data-preprocessing","title":"2 - Data preprocessing","text":"<ul> <li>Split the data into training and testing datasets (e.g., 80% training and 20% testing).</li> <li>Scale the features to a similar range to avoid bias in distance calculations. You can use a method like  min-max scaling or standard scaling. (You can try without !)</li> </ul>"},{"location":"machine-learning/exercice/#3-knn-model-training","title":"3 - KNN model training","text":"<p>Tip</p> <p>Have a look to <code>sklearn.neighbors.KNeighborsClassifier</code>...</p> <ul> <li>Use scikit-learn's KNeighborsClassifier to train a KNN model on the heart rate data.</li> <li>Choose an appropriate value of k (i.e., the number of nearest neighbors to consider) by trying different values and  evaluating the performance of the model using a validation dataset or cross-validation.</li> <li>Fit the model on the training dataset.</li> </ul>"},{"location":"machine-learning/exercice/#4-knn-model-evaluation","title":"4 - KNN Model evaluation","text":"<ul> <li>Evaluate the performance of the KNN model on the test dataset using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC score.</li> <li>Visualize the model's performance using a confusion matrix or ROC curve.</li> <li>Analyze the results and identify areas of improvement for the model.</li> </ul>"},{"location":"machine-learning/exercice/#5-decision-tree-model-training","title":"5 - Decision tree model training","text":"<p>same as <code>3 - KNN model training</code></p> <p>Tip</p> <p>Have a look to <code>sklearn.tree.DecisionTreeClassifier</code>...</p>"},{"location":"machine-learning/exercice/#6-decision-tree-model-evaluation","title":"6 - Decision tree model evaluation","text":"<p>same as <code>4 - KNN Model evaluation</code></p>"},{"location":"machine-learning/exercice/#7-compare-results","title":"7 - Compare results","text":"<p>Tip</p> <p>Just choose one of the model, justify your choice ! </p>"},{"location":"machine-learning/exercice/#bonus-compare-with-other-algorithms","title":"Bonus - Compare with other Algorithms","text":""},{"location":"machine-learning/exercice/#8-send-your-work","title":"8 - Send your work","text":"<p>Danger</p> <p>Your work (the notebook), must be shared in a github repository, you'll send the link to your repo by email.</p>"},{"location":"machine-learning/exercice/#thanks-for-the-dataset","title":"Thanks for the dataset","text":"<pre><code>L. Rachakonda, A. K. Bapatla, S. P. Mohanty, and E. Kougianos, \u201cSaYoPillow: Blockchain-Integrated Privacy-Assured IoMT \nFramework for Stress Management Considering Sleeping Habits\u201d, IEEE Transactions on Consumer Electronics (TCE), Vol. 67,\n No. 1, Feb 2021, pp. 20-29.\n\nL. Rachakonda, S. P. Mohanty, E. Kougianos, K. Karunakaran, and M. Ganapathiraju, \u201cSmart-Pillow: An IoT based Device \nfor Stress Detection Considering Sleeping Habits\u201d, in Proceedings of the 4th IEEE International Symposium on Smart \nElectronic Systems (iSES), 2018, pp. 161--166.\n</code></pre>"},{"location":"machine-learning/intro/","title":"Introduction to Machine Learning","text":"<p>Machine Learning is a rapidly growing field in computer science that focuses on the development of algorithms and models that can learn from and make predictions on data. In this lecture, we will cover the basics of machine learning, including its applications, types of learning, and common algorithms.</p>"},{"location":"machine-learning/intro/#what-is-machine-learning","title":"What is Machine Learning?","text":"<p>Machine Learning (ML) is a subset of Artificial Intelligence (AI) that deals with the ability of a system to learn from data and improve its performance over time. The main goal of ML is to create models that can automatically learn patterns and relationships from data without being explicitly programmed.</p> <p></p>"},{"location":"machine-learning/intro/#and-what-about-deep-learning","title":"And what about Deep Learning ?","text":"<p>Deep learning is a subfield of Machine learning (when you heard about Neural network, it involves deep learning) one key difference between machine learning and deep learning is the level of abstraction involved in the feature  extraction process. In machine learning, features are typically manually extracted from the data and then used as inputs to the model. In deep learning, the model learns to extract the relevant features automatically,  as part of the training process.</p> <p>Question</p> <p>What is feature extraction ? </p> <p>Feature extraction is the process of selecting and transforming raw data into a set of features that can be used to  represent the data in a more meaningful way for a machine learning model. In machine learning, features are  essentially the measurable properties of the data that are used to make predictions or decisions.</p> <p></p>"},{"location":"machine-learning/intro/#why-it-became-so-popular-only-around-2010","title":"Why it became so popular only around 2010?","text":"<p>Machine learning has been around since the mid-20th century, but it wasn't until around 2010 that it became widely  popular. There are several reasons why this happened:</p> <ul> <li> <p>Big Data: The explosion of digital data in the 21st century has made it possible to train and test machine learning  models on massive amounts of data. This has made it easier to develop accurate models that can handle complex tasks.</p> </li> <li> <p>Advances in computing power: The development of powerful computers and the availability of cloud computing resources  have made it possible to process and analyze large amounts of data quickly and efficiently.</p> </li> <li> <p>Open-source software: The availability of open-source software frameworks like TensorFlow, PyTorch, and Scikit-learn  has made it easier for developers to experiment with machine learning algorithms and develop applications.</p> </li> <li> <p>Increased awareness: The growth of social media and the internet has led to increased awareness of the potential  applications of machine learning, and its ability to solve complex problems.</p> </li> </ul>"},{"location":"machine-learning/intro/#example-applications-of-machine-learning","title":"Example applications of Machine Learning","text":"<p>Machine Learning is being used in various applications across different industries. Some of the popular applications include:</p> <ul> <li>Image and Speech Recognition</li> <li>Natural Language Processing</li> <li>Fraud Detection</li> <li>Recommender Systems</li> <li>Predictive Maintenance</li> <li>Autonomous Vehicles</li> </ul>"},{"location":"machine-learning/intro/#types-of-machine-learning","title":"Types of Machine Learning","text":"<p>There are three main types of Machine Learning : </p>"},{"location":"machine-learning/intro/#supervised-learning","title":"Supervised Learning","text":"<p>In supervised learning, the model is trained on labeled data (input-output pairs). The goal is to learn a mapping function that can predict the output for new input data. Popular algorithms include Linear Regression, Logistic Regression, Decision Trees, Random Forests, and Neural Networks.</p>"},{"location":"machine-learning/intro/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>In unsupervised learning, the model is trained on unlabeled data. The goal is to learn the underlying structure of the data and identify patterns and relationships. Popular algorithms include Clustering, Principal Component Analysis (PCA), and Association Rule Learning.</p>"},{"location":"machine-learning/intro/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>In reinforcement learning, the model learns by interacting with the environment and receiving feedback in the form of rewards or punishments. The goal is to learn a policy that maximizes the cumulative reward over time. Popular algorithms include Q-Learning, SARSA, and Deep Reinforcement Learning.</p> <pre><code>\n    graph LR\n    A[Machine Learning] --&gt; B[Supervised Learning]\n    A --&gt; C[Unsupervised Learning]\n    A --&gt; D[Reinforcement Learning]\n    B --&gt; E[Regression]\n    B --&gt; F[Classification]\n    C --&gt; G[Clustering]\n    C --&gt; H[Dimensionality Reduction]\n</code></pre>"},{"location":"machine-learning/intro/#some-examples-foreach-category","title":"Some examples foreach category","text":""},{"location":"machine-learning/intro/#supervised-learning_1","title":"Supervised Learning:","text":"<p>Here are some examples of applications of supervised learning:</p> <ul> <li>Spam detection: In email filtering, the algorithm is trained on labeled data to classify emails as spam or not spam. </li> <li>Image recognition: In image classification, the algorithm is trained on labeled images to identify objects within images, such as people or animals. </li> </ul> <p>Data labeling information</p> <p>Maybe you know it, but when you're doing a captcha... you're labelling image</p> <p></p> <p>Facebook's photo tagging system is one example of how the company uses machine learning to identify faces in photos  and suggest tags to users. When users tag their friends in photos, it helps Facebook's algorithms learn and improve  their ability to identify faces and make more accurate suggestions in the future.</p> <p>Similarly, Twitter uses hashtags to train their algorithms. When users include hashtags in their tweets,  it helps Twitter's machine learning algorithms understand the context and topics being discussed.  This enables Twitter to make better recommendations to users about who to follow, what content to engage with,  and which ads to display.</p> <p></p> <ul> <li>Object detection is a computer vision task that involves identifying and localizing objects within an image or video. It is a more complex task than image classification because it not only requires identifying what objects are present in an image, but also where they are located within the image. Object detection algorithms are typically trained on labeled images that provide both the class label  (e.g. \"person\", \"car\", \"tree\", etc.) and the bounding box coordinates of each object in the image. </li> </ul> <p></p> <ul> <li>Fraud detection: In financial fraud detection, the algorithm is trained on labeled data to identify fraudulent transactions.</li> </ul>"},{"location":"machine-learning/intro/#unsupervised-learning_1","title":"Unsupervised Learning:","text":"<p>Here are some examples of applications of unsupervised learning:</p> <ul> <li>Customer segmentation: In marketing, unsupervised learning can be used to segment customers into groups based on similar behavior or characteristics.</li> <li>Anomaly detection: In cybersecurity, unsupervised learning can be used to identify unusual network activity or behavior that may be indicative of a security breach.</li> <li>Topic modeling: In natural language processing, unsupervised learning can be used to identify topics within a corpus of text documents.</li> </ul>"},{"location":"machine-learning/intro/#reinforcement-learning_1","title":"Reinforcement Learning:","text":"<p>Here are some examples of applications of reinforcement learning:</p> <ul> <li>Game playing: Reinforcement learning has been used to develop game-playing agents that can beat human experts at games such as chess and Go.</li> <li>Robotics: Reinforcement learning can be used to train robots to perform tasks such as grasping and manipulation of objects.</li> <li>Autonomous driving: Reinforcement learning can be used to train self-driving cars to navigate complex traffic scenarios.</li> </ul>"},{"location":"machine-learning/intro/#common-machine-learning-algorithms","title":"Common Machine Learning Algorithms","text":"<p>There are various algorithms used in Machine Learning, some of which include:</p> Algorithm Category Linear Regression Supervised Learning Logistic Regression Supervised Learning Decision Trees Supervised Learning Random Forest Supervised Learning Support Vector Machines (SVM) Supervised Learning k-Nearest Neighbors (k-NN) Supervised Learning Naive Bayes Supervised Learning K-Means Unsupervised Learning Hierarchical Clustering Unsupervised Learning Principal Component Analysis (PCA) Unsupervised Learning Apriori Unsupervised Learning QLearning Reinforcement Learning <p>Note</p> <p>This list contains only some of the common algorithm you'll met</p>"},{"location":"machine-learning/intro/#some-history","title":"Some history","text":""},{"location":"machine-learning/intro/#conclusion","title":"Conclusion","text":"<p>In conclusion, Machine Learning is an exciting field that has numerous applications and is rapidly growing. In this lecture, we have covered the basics of Machine Learning, including its applications, types of learning, and common algorithms. In the following lectures, we will dive deeper into each of these topics and learn how to apply them in real-world scenarios.</p>"},{"location":"machine-learning/knn/","title":"K-Nearest-neighbor (KNN)","text":"<p>K-nearest neighbors is a type of machine learning algorithm that can be used for both <code>regression</code> and <code>classification</code>  problems. The algorithm works by finding the <code>K</code> number of training examples that are closest to the new data point,  and then using the labels of those examples to make a prediction for the new data point.</p> <p>Classification with KNN</p> <p></p> <p>What is \"K\" ?</p> <p>The \"K\" in K-nearest neighbors refers to the number of neighbors that the algorithm uses to make its prediction.  For example, if K=3, then the algorithm will find the three training examples that are closest to the new data point, and use the labels of those examples to predict the label of the new data point.</p> <p>How does KNN works?</p> <p>The algorithm determines which training examples are closest to the new data point by calculating the distance between  the new data point and each of the training examples in the feature space. The most common distance metric used is  <code>Euclidean distance</code>, but other distance metrics can be used as well.</p> <p>Once the K nearest neighbors have been identified, the algorithm will use those neighbors to make a prediction for the  new data point. If the problem is a classification problem, the algorithm will use the mode (most common) label of the  K neighbors as the predicted label for the new data point. If the problem is a regression problem, the algorithm will  use the mean or median of the K neighbors as the predicted value for the new data point.</p> <p>Is the value of K important ?</p> <p>It's worth noting that the choice of K can have a significant impact on the performance of the algorithm. A small value  of K (e.g., K=1) can lead to overfitting, where the algorithm memorizes the training data and doesn't generalize well to new data. A large value of K (e.g., K=n, where n is the number of training examples) can lead to underfitting, where the algorithm doesn't capture the underlying structure of the data.</p> <p>Overall, K-nearest neighbors is a simple and effective machine learning algorithm that can be used for a variety of  problems. However, it can be sensitive to the choice of distance metric and the value of K, and may not perform as well  as other more sophisticated algorithms for certain types of data.</p>"},{"location":"machine-learning/knn/#type-of-distance","title":"Type of distance","text":"Distance Metric Formula Description Euclidean distance \\(\\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\\) Measures the straight-line distance between two points in a feature space. Manhattan distance \\(\\sum_{i=1}^{n}\\lvert x_i - y_i \\rvert\\) Measures the sum of the absolute differences between the coordinates of two points in a feature space. Minkowski distance (p=3) \\(\\sqrt[3]{\\sum_{i=1}^{n}(x_i - y_i)^3}\\) A generalization of both Euclidean distance and Manhattan distance that can be used for any value of p. Cosine similarity \\(\\frac{\\sum_{i=1}^{n}(x_i \\times y_i)}{\\sqrt{\\sum_{i=1}^{n}x_i^2} \\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\\) Measures the cosine of the angle between two vectors in a feature space, which is commonly used for text classification and recommendation systems. Hamming distance \\(\\sum_{i=1}^{n}[x_i \\neq y_i]\\) Measures the number of positions at which two binary strings differ, which is commonly used for string matching and error correction."},{"location":"machine-learning/linear-regression/","title":"Linear regression","text":""},{"location":"machine-learning/linear-regression/#simple-linear-regression","title":"Simple linear regression","text":"<p>What is a simple linear regression?</p> <p>Simple linear regression is a statistical method used to model the relationship between two variables,  where one variable (the independent variable) is used to predict the other variable (the dependent variable).  The goal of simple linear regression is to find a line of best fit that summarizes the relationship between the two  variables. In other words, we want to find the equation of a line that comes as close as possible to passing through  all of the data points in the scatter plot of the two variables.</p>"},{"location":"machine-learning/linear-regression/#lets-start-by-an-example","title":"Let's start by an example","text":""},{"location":"machine-learning/linear-regression/#input-data","title":"Input data","text":"<p>Assuming we have a dataset which represents the given tips depending on the amount of the bill</p> <p>Info</p> <p>We already know some data</p> <p></p> <p>Info</p> <p>From this data, we can find <code>y = mx + b</code> that \"best fit\" the points</p> <p></p> <p>Info</p> <p>Now we can predict <code>tips_amount</code> given <code>total_bill</code></p> <p></p> <p>The equation for a straight line is given by:</p> \\[ {y = mx + b} \\] <p>Where :</p> <ul> <li>y is the dependent variable </li> <li>x is the independent variable</li> <li>m is the slope of the line</li> <li>b is the y-intercept (i.e., the value of y when x is zero).</li> </ul> <p>How to find the values of m and b?</p> <p>In simple linear regression, we use the method of least squares to find the values of <code>m</code> and <code>b</code> that minimize the  sum of the squared differences between the observed values of <code>y</code> and the predicted values of <code>y</code>  (i.e., the values of y predicted by the line of best fit).</p> <p>The least squares method involves the following steps:</p> <ul> <li>Compute the mean of the independent variable x and the mean of the dependent variable y.</li> <li> <p>Compute the slope of the line of best fit m using the formula: </p> <p><code>m = sum((xi - x_mean) * (yi - y_mean)) / sum((xi - x_mean)^2)</code></p> <p>Where : </p> <ul> <li>xi and yi are the values of the independent and dependent variables, respectively </li> <li>x_mean and y_mean are their respective means.</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Compute the y-intercept b using the formula:</p> <p><code>b = y_mean - m * x_mean</code></p> </li> </ul> <p>Once we have computed the values of m and b, we can use the equation <code>y = mx + b</code> to predict the value of y for any  given value of x.</p> <p>In the case of multiple linear regression, where there are multiple independent variables, the computation is a bit more complicated, but the basic idea is the same: we want to find the equation of a hyperplane that comes as close as  possible to passing through all of the data points in the scatter plot of the independent and dependent variables.  The method of least squares is also used to find the values of the coefficients (i.e., the slopes) of the hyperplane  that minimize the sum of the squared differences between the observed values of y and the predicted values of y.</p>"},{"location":"machine-learning/linear-regression/#implementation-example-with-sklearn","title":"Implementation example with Sklearn","text":""},{"location":"machine-learning/logistic-regression/","title":"Logistic regression","text":"<p>What is a logistic regression</p> <p>Logistic regression is a statistical method used for modeling the relationship between a binary dependent variable  (also known as the response or outcome variable) and one or more independent variables (also known as predictors  or explanatory variables). The goal of logistic regression is to predict the probability of the binary outcome  based on the values of the predictors.</p> <p>The dependent variable in logistic regression is typically binary (e.g., yes/no, 1/0, success/failure), although it can also be ordinal or nominal. The independent variables can be continuous, categorical, or a combination of both.</p> <p>The logistic regression model estimates the log odds (logit) of the binary outcome as a linear combination of the independent variables. The logit is then transformed into a probability using the logistic function, which produces a value between 0 and 1. This probability represents the predicted likelihood of the binary outcome given the values of the independent variables.</p> <p>Logistic regression is often used in classification tasks, such as predicting whether a customer will churn or not, whether an email is spam or not, or whether a patient will respond to a particular treatment or not. It can also be used for understanding the relationship between variables and exploring the effects of different factors on the binary outcome.</p> <p>The math behind</p> <p>The mathematical foundation of logistic regression is based on the logistic function, which is a type of sigmoid  function. The logistic function is defined as:</p> \\[f(z) = 1 / (1 + e^-z)\\] <p>where z is the linear combination of the independent variables and their associated coefficients, or weights. In other words, z = b0 + b1x1 + b2x2 + ... + bpxp, where b0 is the intercept or constant term and b1 to bp are the coefficients for the independent variables x1 to xp.</p>"},{"location":"machine-learning/logistic-regression/#the-iris-dataset-example","title":"The iris dataset example","text":"<p>The iris dataset is often used for educational purpose in machine learning.  It contains the following informations : </p> Column Description sepal_length Length of sepal (in centimeters) sepal_width Width of sepal (in centimeters) petal_length Length of petal (in centimeters) petal_width Width of petal (in centimeters) species The species of iris (Setosa, Versicolor, or Virginica) <p>Objective</p> <p>Given the four feature sepal_length, sepal_width, petal_length, petal_width, we want to build a model which is able  to predict the class (which species) based on new samples -&gt; It's a multi class classification problem</p>"},{"location":"machine-learning/logistic-regression/#lets-deep-dive-a-bit-in-the-data","title":"Let's deep dive a bit in the data","text":"<p>Info</p> <p>The example we will use contains 150 samples</p> <p></p>"},{"location":"machine-learning/logistic-regression/#now-lets-solve-this-classification-problem-with-the-logistic-regression","title":"Now let's solve this classification problem with the Logistic Regression","text":""},{"location":"machine-learning/model-evaluation/","title":"Model evaluation","text":""},{"location":"machine-learning/model-evaluation/#splitting-our-data-for-training-testing-and-validation","title":"Splitting our data for training, testing, and validation","text":"<p>Info</p> <p>Train-test-validation is a common approach used in machine learning to evaluate the performance of a model.  The approach involves dividing a dataset into three parts: a training set, a validation set, and a test set.</p>"},{"location":"machine-learning/model-evaluation/#the-training-set","title":"The training set","text":"<p>It is the portion of the dataset used to train the model. The model learns from the patterns and  relationships present in the training data and uses this knowledge to make predictions on new, unseen data.</p>"},{"location":"machine-learning/model-evaluation/#the-validation-set","title":"The validation set","text":"<p>It is used to evaluate the performance of the model during training.  As the model learns from the training data, its performance is evaluated on the validation set to assess whether  it is overfitting (i.e., memorizing the training data and performing poorly on new data) or underfitting  (i.e., not capturing the patterns and relationships present in the training data).</p>"},{"location":"machine-learning/model-evaluation/#the-test-set","title":"The test set","text":"<p>Tips</p> <p>You can think about the test set as <code>unseen data</code></p> <p>It is used to evaluate the performance of the model after it has been trained and tuned on the training and  validation sets. The test set provides an unbiased estimate of the model's performance on new, unseen data.</p> <p>The train-test-validation approach is important because it enables us to assess the quality of the model's predictions and make informed decisions about how to improve the model. By dividing the dataset into separate training, validation, and test sets, we can ensure that the model is not overfitting or underfitting and that it can generalize well to new, unseen data.</p>"},{"location":"machine-learning/model-evaluation/#cross-validation","title":"Cross-validation","text":"<p>What is cross validation ?</p> <p>Cross-validation is a technique used to evaluate the performance of a machine learning model on a limited dataset by  splitting the data into multiple subsets, training the model on some subsets, and evaluating its performance on the  remaining subsets.</p> <p>The basic idea behind cross-validation is to ensure that the model is not overfitting the data and can generalize well  to new data. It does this by simulating the process of testing the model on new data that it has not seen before.</p> <p>The most common form of cross-validation is k-fold cross-validation, where the data is divided into k equal-sized  subsets, or folds. The algorithm is then trained on k-1 folds and tested on the remaining fold. This process is  repeated k times, with each fold used exactly once as the testing data. The results from each fold are then averaged  to obtain an estimate of the model's performance.</p> <p>For example, in 5-fold cross-validation, the dataset would be split into five equal parts, with four parts used  for training and one part used for testing. This process is repeated five times, with each of the five parts used as the testing data once. The average of the results from the five rounds is then used as an estimate of the model's  performance.</p> <p></p> <p>One of the advantages of cross-validation is that it can provide a more accurate estimate of a model's performance  than a single train-test split. This is because the results are averaged over multiple rounds, reducing the variability of the estimate. It can also help to detect overfitting, as a model that overfits the training data will perform poorly on the testing data.</p> <p>Warning</p> <p>Cross-validation can also be computationally expensive, as it requires training and evaluating the model  multiple times. It may also be less appropriate for some datasets, such as those with highly imbalanced classes or  temporal dependencies.</p> <p>Overall, cross-validation is a powerful tool for evaluating the performance of a machine learning model and can help to improve the accuracy and generalization of the model.</p>"},{"location":"machine-learning/model-evaluation/#classification","title":"Classification","text":"<p>Tips</p> <p>In general we use a confusion matrix to visualize the error made by an algorithm in a classification problem it's not limited to binary classification.</p> Actual Positive Actual Negative Predicted Positive TP FP Predicted Negative FN TN Evaluation Metric Formula When to Use Accuracy (TP + TN) / (TP + TN + FP + FN) Balanced Precision TP / (TP + FP) Imbalanced Recall TP / (TP + FN) Imbalanced F1 Score 2 * (Precision * Recall) / (Precision + Recall) Imbalanced"},{"location":"machine-learning/model-evaluation/#regression","title":"Regression","text":"Evaluation Metric Formula When to Use Mean Squared Error (MSE) 1/n * \u2211(y_true - y_pred)^2 General Root Mean Squared Error (RMSE) \u221a(1/n * \u2211(y_true - y_pred)^2) General R-squared (R2) 1 - \u2211(y_true - y_pred)^2 / \u2211(y_true - y_true_mean)^2 General MAE (Mean Absolute Error) 1/n * \u2211                        (      y_true - y_pred ) General MAPE (Mean Absolute Percentage Error) 100% * 1/n * \u2211( y_true - y_pred) / y_true Specific"},{"location":"machine-learning/prerequisites/","title":"Before developing","text":"<p>Warning</p> <p>Before creating a machine learning algorithm, there are several steps that should be taken to ensure that the  problem is well-defined, the data is appropriate and that the algorithm can be effectively trained</p>"},{"location":"machine-learning/prerequisites/#1-define-the-problem","title":"1 - Define the problem","text":"<p>Start by defining the problem you want to solve, the objectives. </p> <ul> <li>What is the goal of your algorithm? </li> <li>What kind of data will you be working with?</li> <li>What performance are we expecting ?</li> <li>Do I have data ? </li> </ul>"},{"location":"machine-learning/prerequisites/#2-collect-the-data-the-right-data","title":"2 - Collect the data... The right data","text":"<p>Gather the data you will need to train and test your algorithm. This might involve:</p> <ul> <li>scraping data from the web</li> <li>collecting data from sensors</li> <li>working with pre-existing data sets</li> </ul> <p>Danger</p> <p>In real-life application, the data itself is one thing on which you need to spend time :  </p> <ul> <li>You need to ensure that the data describe your real-life problem </li> <li>You need to ensure that its quality is good</li> <li>Are you able to produce the data you will use to train your model It doesn't make sense to have an algorithm that work well in laboratory condition, if you need to use it in  different condition</li> </ul> <p>Let's take an example :</p> <p>Example</p>"},{"location":"machine-learning/prerequisites/#assuming-you-want-to-train-a-classification-model-to-classify-eyes-diseases-with-a-camera-embedded-on-a-raspberry","title":"Assuming you want to train a classification model to classify eyes diseases, with a camera embedded on a raspberry.","text":"<p>After some research on google, you'll find some already labeled dataset like this :</p> <p></p> <p> </p> <p>But with your camera, you're only able to produce image like this : </p> <p></p> <p>It's completly out of scope, it doesn't make sense to train the algorithm on this dataset</p>"},{"location":"machine-learning/prerequisites/#what-can-i-do","title":"What can i do ?","text":"<p>You have mainly two solutions : </p> <ul> <li>Take some pictures of eyes with diseases you want to classify</li> <li>Scrap some data on internet and label it yourself</li> </ul>"},{"location":"machine-learning/prerequisites/#3-explore-the-data","title":"3 - Explore the data","text":"<p>Once you have your data, explore it to get a better understanding of what you're working with.  This might involve data visualization and basic statistical analysis.</p> <p>See an example</p>"},{"location":"machine-learning/prerequisites/#4-preprocess-the-data","title":"4 - Preprocess the data","text":"<p>Preprocessing the data involves transforming and cleaning the data so that it can be used by the machine learning  algorithm. This might involve feature engineering, data scaling, data normalization, data augmentation and data  cleaning.</p> What is data normalization ? <p>Data normalization is the process of rescaling the values of numeric features in a dataset to a common scale.  This is typically done to prevent features with large values from dominating the analysis and to ensure that all  features have equal weight. One common technique for normalization is called <code>min-max scaling</code>  which involves scaling the values of a feature to a range between 0 and 1 by subtracting the minimum value of the  feature and dividing by the range of the feature.</p> What is data scaling ? <p>Data scaling is a similar technique to normalization that involves transforming the values of numeric features  so that they have a mean of 0 and a standard deviation of 1. This is typically done to ensure that all features  have the same variance and that the data is centered around 0. One common technique for scaling is called  \"standardization,\" which involves subtracting the mean of the feature and dividing by its standard deviation.</p> What is data augmentation ? <p>Data augmentation is a technique used to increase the size of a training dataset by creating new, <code>synthetic</code> data points through transformations of existing data. The goal of data augmentation is to improve the  performance and generalization of machine learning models by exposing them to a wider range of variations in the data.</p> <p>Data augmentation can be applied to a wide range of data types, including images, audio, and text. For example, in image data augmentation, you can create new images by applying transformations such as rotation, translation, scaling,  flipping, and cropping to existing images. In audio data augmentation, you can apply transformations such as changing the pitch, tempo, or volume of existing audio recordings. In text data augmentation, you can create new text samples by replacing words with synonyms.</p> <p>Data augmentation is particularly useful in scenarios where the amount of available training data is limited. By creating new data points through data augmentation, you can effectively increase the size of the training set, which can help to prevent overfitting and improve the generalization of machine learning models.</p> <p>Example</p> <p></p> What is data cleaning ? <p>Data cleaning is a critical step in the data preprocessing pipeline that involves identifying and correcting errors,  inconsistencies, and missing or irrelevant data in a dataset. The goal of data cleaning is to prepare the data for  analysis or machine learning by ensuring that it is accurate, complete, and consistent.</p>"},{"location":"machine-learning/prerequisites/#data-cleaning-typically-involves-a-series-of-steps-which-may-include","title":"Data cleaning typically involves a series of steps, which may include:","text":""},{"location":"machine-learning/prerequisites/#missing-data-handling","title":"Missing data handling","text":"<p>If there are missing values in the dataset, data cleaning may involve imputing missing values  using techniques such as mean imputation, median imputation, or regression imputation.</p>"},{"location":"machine-learning/prerequisites/#outlier-detection-and-handling","title":"Outlier detection and handling","text":"<p>Outliers are data points that fall far outside the normal range of values in the  dataset. Data cleaning may involve detecting and handling outliers using techniques such as box plots, z-scores, or clustering.</p>"},{"location":"machine-learning/prerequisites/#data-formatting-and-type-conversion","title":"Data formatting and type conversion","text":"<p>Data cleaning may involve converting data types, such as converting categorical data to numerical data, or converting date formats to a standardized format.</p>"},{"location":"machine-learning/prerequisites/#removing-duplicates","title":"Removing duplicates","text":"<p>If there are duplicate records in the dataset, data cleaning may involve identifying and removing them to avoid biases in the analysis.</p>"},{"location":"machine-learning/prerequisites/#data-validation-and-verification","title":"Data validation and verification","text":"<p>Data cleaning may involve validating the data to ensure that it is accurate and  complete, such as cross-checking data against external sources or conducting manual spot-checks.</p>"},{"location":"machine-learning/resources/","title":"Some additional resources","text":"<ul> <li>https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/super-cheatsheet-machine-learning.pdf</li> <li>https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-machine-learning-tips-and-tricks.pdf</li> </ul>"},{"location":"machine-learning/tooling/","title":"Tooling we will use","text":""},{"location":"machine-learning/tooling/#language","title":"Language","text":"<p>Python</p>"},{"location":"machine-learning/tooling/#a-platform","title":"A platform","text":"<p>Google colab</p> <p>It's a cloud-based platform for developing and running machine learning models. It is a free service provided by Google  that allows users to write and run Python code in a Jupyter notebook environment, with access to powerful computing  resources and pre-installed libraries and frameworks for machine learning.</p> <p>Go to google colab</p>"},{"location":"machine-learning/tooling/#machine-learning-libraries","title":"Machine learning libraries","text":"<ul> <li>scikit-learn</li> <li>TensorFlow</li> <li>Keras</li> </ul> <p>These libraries provide pre-built functions and models for various machine learning tasks, making it easier to develop  machine learning models.</p>"},{"location":"machine-learning/tooling/#data-visualization-libraries","title":"Data visualization libraries","text":"<ul> <li>matplotlib </li> <li>seaborn</li> </ul>"},{"location":"machine-learning/tooling/#data-manipulation-libraries","title":"Data manipulation libraries","text":"<ul> <li>pandas</li> <li>numpy</li> </ul>"},{"location":"sorting-algorithm/bubble-sort/","title":"Bubble sort","text":""},{"location":"sorting-algorithm/bubble-sort/#bubble-sort","title":"Bubble sort","text":"<p>Bubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.</p> <p>Warning</p> <p>Bubble sort is not suitable to sort large datasets</p> <p>Implementation</p> GoPython <pre><code>func bubbleSort(integers []int) []int {\nnbElements := len(integers)\nfor i := 0; i &lt; nbElements-1; i++ {\nfor j := 0; j &lt; nbElements-i-1; j++ {\nif integers[j] &gt; integers[j+1] {\nintegers[j], integers[j+1] = integers[j+1], integers[j]\n}\n}\n}\nreturn integers\n}\n</code></pre> <pre><code>def bubble_sort():\npass\n</code></pre> <p>Time complexity</p> \\[ {O(n^2)} \\] <p>Space complexity \\({O(1)}\\)</p>"},{"location":"sorting-algorithm/insertion-sort/","title":"Insertion sort","text":""},{"location":"sorting-algorithm/insertion-sort/#insertion-sort","title":"Insertion sort","text":"<p>Insertion sort is a simple sorting algorithm that builds the final sorted array one item at a time. It iterates through an input list, compares adjacent elements and inserts the current element into the correct position in the sorted list.</p> <p>Warning</p> <p>Insertion sort is not suitable to sort large datasets</p> <p>Implementation</p> GoPython <pre><code>func insertionSort(integers []int) []int {\nnbElements := len(integers)\nfor i := 0; i &lt; nbElements; i++ {\nvalueToInsert := integers[i]\ncurrentPosition := i\nfor currentPosition &gt; 0 &amp;&amp; integers[currentPosition-1] &gt; valueToInsert {\nintegers[currentPosition] = integers[currentPosition-1]\ncurrentPosition = currentPosition - 1\n}\nintegers[currentPosition] = valueToInsert\n}\nreturn integers\n}\n</code></pre> <pre><code>def insertion_sort():\npass\n</code></pre> <p>Time complexity</p> <p>Space complexity</p>"},{"location":"sorting-algorithm/selection-sort/","title":"Selection sort","text":""},{"location":"sorting-algorithm/selection-sort/#selection-sort","title":"Selection sort","text":"<p>Selection sort is an in-place comparison sorting algorithm that divides the input list into two parts: the sublist of items already sorted, and the sublist of items remaining to be sorted. It finds the minimum element in the unsorted sublist and swaps it with the leftmost unsorted element, moving the sublist boundaries one element to the right.</p> <p>Warning</p> <p>Selection sort is not suitable to sort large datasets</p>"},{"location":"sorting-algorithm/selection-sort/#implementation","title":"Implementation","text":"GoPython <pre><code>func selectionSort(integers []int) []int {\nnbElements := len(integers)\nfor i := 0; i &lt; nbElements-1; i++ {\nminIndex := i\nfor j := i; j &lt; nbElements; j++ {\nif integers[j] &lt; integers[minIndex] {\nminIndex = j\n}\n}\nintegers[i], integers[minIndex] = integers[minIndex], integers[i]\n}\nreturn integers\n}\n</code></pre> <pre><code>def selection_sort():\npass\n</code></pre>"},{"location":"sorting-algorithm/selection-sort/#time-complexity","title":"Time complexity","text":""},{"location":"sorting-algorithm/selection-sort/#space-complexity","title":"Space complexity","text":"<p>Note</p> <p>Selection sort is useful when memory usage is a concern, as it is an in-place sorting algorithm that does not require additional memory. It is also useful when you only need to sort a small number of items, and performance is not a critical concern. Selection sort can be used when the list is partially sorted or when the order of equal elements is not important.</p>"}]}